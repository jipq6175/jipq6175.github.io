<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Walking through score-based diffusion with SDE | Yen-Lin Chen</title> <meta name="author" content="Yen-Lin Chen"> <meta name="description" content="Some derivations and calculations of DSM with SDE"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jipq6175.github.io/blog/2023/score_diffusion/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yen-Lin </span>Chen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Walking through score-based diffusion with SDE</h1> <p class="post-meta">September 9, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/reading"> <i class="fas fa-hashtag fa-sm"></i> reading</a>   <a href="/blog/tag/solving"> <i class="fas fa-hashtag fa-sm"></i> solving</a>     ·   <a href="/blog/category/models"> <i class="fas fa-tag fa-sm"></i> models</a>   </p> </header> <article class="post-content"> <h3 id="0-diffusion-model-using-score-matching-and-sde">0. Diffusion Model using Score Matching and SDE</h3> <p>Score matching is a technique that is used for the score-based genertive models, which regress on the scores, \(\nabla_x\log p(x)\) instead of modeling the original data distribution \(p(x)\). The score is the gradient of the data distribution so if we can access this gradient on every point of the data space, we can just follow this gradient and achieve the points with large probability density. This can be achieved by gradient ascend or Langevin dynamics which is similar to the stochastic gradient ascend.</p> \[x_{t+1} = x_t + \epsilon\nabla_x\log p(x) + \sqrt{2\epsilon} z\] <p>where \(z~N(0, I)\). The goal of training the score-based generative model is to approximate the scores using a neural network \(s_\theta(x) \approx \epsilon\nabla_x\log p(x)\) everywhere on \(x\). If we have such model, we can use the above Langevin dynamics for sampling, just replacing the score with model estimates:</p> \[x_{t+1} = x_t + \epsilon s_\theta(x) + \sqrt{2\epsilon} z\] <p>The score-matching objectve (or loss function) is the Fischer divergence between \(s_\theta(x)\) and \(\nabla_x\log p(x)\), weighted by \(p(x)\).</p> \[\mathbb{E}_{p(x)}||s_\theta(x) - \nabla_x\log p(x)||_2^2 = \int p(x)||s_\theta(x) - \nabla_x\log p(x)||_2^2\] <p>The main issue with this simple score-matching is that in regions where \(p(x) \approx 0\), the score extimate will be inaccurate due to the zero weighting in these regions. So if we start our sample in such region and try to follow the score (or gradient), we are doing some random walk and will never get closer to the modes of data distribution. This is more severe in the high-dimensional case where data distribution is like a bunch of spikes (delta functions) in the data space.</p> <p>The trick to solve this if to introduce noise to the data distribution, trying to widen the distribution and cover as much space as possible. The introduction of the noise will result in non-zero \(p_{\sigma_i}(\tilde{x}\mid x)\), where \(\tilde{x}\) is the noised data controlled by variance \(\sigma_i^2\), and cover larger data space for more accurate score estimate.</p> <p><br></p> <h4 id="score-matching-with-langevin-dynamics-smld">Score Matching with Langevin Dynamics (SMLD)</h4> <p>One way is to add different noises with increasing variances \(\{\sigma_0^2, \sigma_1^2, ..., \sigma_N^2\}\), as proposed in the Noise Conditional Score Network (NCSN). The network \(s_\theta(x)\) now need to be noise-conditioned, \(s_\theta(x, \sigma)\) in order to match the scores at diferent variances. The NSCN objective is now</p> \[\sum_{i=1}^N \sigma_i^2 \mathbb{E}_{p(x)}\mathbb{E}_{p_{\sigma_i}(\tilde{x} \mid x)} || s(\tilde{x}, \sigma_i) - \nabla_{\tilde{x}}\log p_{\sigma_i}(\tilde{x}\mid x) ||_2^2\] <h4 id="denoising-diffusion-probabilistic-model-ddpm">Denoising Diffusion Probabilistic Model (DDPM)</h4> <p>Another way is to add noise via a discrete Markov chain, \(p(x_i \mid x_{i-1}) = \mathcal{N}(x_i, \sqrt{1-\beta_i}x_{i-1}, \beta_i I)\). Notice that this Markov chain attenuates the signal and adds noise, instead of just adding noise to overwhelm the signal as done in SMLD. The DDPM objective is now</p> \[\sum_{i=1}^N (1-\alpha_i) \mathbb{E}_{p(x)}\mathbb{E}_{p_{\alpha_i}(\tilde{x}\mid x)} || s(\tilde{x}, i) - \nabla_{\tilde{x}}\log p_{\alpha_i}(\tilde{x}\mid x) ||_2^2\] <p>Notice the similarity between the objectives of SMLD and DDPM.</p> <p><br></p> <h3 id="1-general-continuous-time-diffusion">1. General Continuous-Time Diffusion</h3> <p>The SMLD and DDPM were descrete-time, with a pre-defined variance schedule: \(\sigma_i^2\) for SMLD and \(\beta_i\) for DDPM. A general case of adding noise to data is to use stochastic differential equations (SDEs), which involves adding gaussian noise (or Brownian motion).</p> \[dx = f(x, t)dt + g(t)dw\] <p>where \(f(x, t)\) is the drift term that depends on current \(x\) and time \(t\). \(g(t)\) is the diffusion term, controlling how much noise to add to the data at certain time \(t\) and \(dw\) is the infinitesimal Brownian motion. With this predefined stochastic process, the time reveral of the SDE has close form</p> \[dx = [f(x, t) - g^2(t) \nabla_x\log p_t(x)]dt + g(t)dw\] <p>Note that here, in the time reversal, \(dt &lt; 0\), so we are actually reversing the drift and following along the gradient (in the same direction of the score). We now just need to have a time-conditioned score model \(s_\theta(x, t)\) that matches \(\nabla_x\log p_t(x)\) everywhere, everytime.</p> <p>There is also a guarantee that the distribution of \(x(t)\) following this SDE, is a normal distribution with mean \(m(t)\) and variance \(v(t)\). So we can write down the perturbation kernel or transitional kernel from data \(x(0)\) to noised data \(x(t)\):</p> \[p_{0t}\left(x(t) | x(0) \right) = \mathcal{N}\left(x(t); m(t), v(t)I \right)\] <p>We are just writing this down for derivation of \(m(t)\) and \(v(t)\) later. Note here that the data is actually multi-dimensional, but each dimensional is treated as independent, so we can just treat everything in scalar form and write $I$ for the variance.</p> <p>The denoising score matching objective is now</p> \[\mathcal{L}_{dsm} = \mathbb{E}_t \lambda(t) \mathbb{E}_{x(0)} \mathbb{E}_{x(t)\mid x(0)} || s_\theta(x(t), t) - \nabla_{x(t)}\log p_{0t}\left(x(t) \mid x(0) \right)||_2^2\] <p>\(\lambda(t)\) is the positive time-dependent weighting and is proportional to the variance squared \(v^2(t)\) as done in SMLD and DDPM. In the maximum likelihood training proposed later, \(\lambda(t)\) is proportional to the diffusion term squared \(g^2(t)\).</p> <p>The gradient term can be easily calculated in exact form since \(p_{0t}\) is a gaussian:</p> \[\nabla_{x(t)}\log p_{0t}\left(x(t) \mid x(0) \right) = \nabla_{x(t)}\log \mathcal{N}\left(x(t); m(t), v(t)I \right) = -\frac{x(t) - m(t)}{v(t)}\] <p>How do we compute \(x(t)\) in practice? Again, using \(p_{0t}\):</p> \[x(t) = m(t) + \sqrt{v(t)}z; z\sim \mathcal{N}(z; 0, I)\] <p>Plug \(x(t) - m(t) = \sqrt{v(t)}z\) above and then \(\mathcal{L}_{dsm}\) yields:</p> \[\mathcal{L}_{dsm} = \mathbb{E}_t \lambda(t) \mathbb{E}_{x(0)} \mathbb{E}_{x(t)\mid x(0)} \left|\left| s_\theta(x(t), t) + \frac{z}{\sqrt{v(t)}}\right|\right|_2^2\] <p>The critical components are \(m(t)\) and \(v(t)\). Once we have them, we can compute the loss and train the score model.</p> <p><br></p> <h4 id="sde-for-smld-ve-sde">SDE for SMLD (VE-SDE)</h4> <p>The discrete-time Markov chain for SMLD is \(x_i = x_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} z_{i-1}\). In the continuous-time generalization:</p> \[x(t+\Delta t) = x(t) + \sqrt{\sigma^2(t+\Delta t) - \sigma^2(t)}z(t) \approx x(t) + \sqrt{\frac{d\sigma^2(t)}{dt}\Delta t}z(t)\] <p>We combine \(\sqrt{\Delta t}z(t) = dw\). The continuous-time SMLD is then</p> \[dx = \sqrt{\frac{d\sigma^2(t)}{dt}}dw\] <p>which is also called variance-exploding SDE (VE-SDE).</p> <h4 id="sde-for-ddpm-vp-sde">SDE for DDPM (VP-SDE)</h4> <p>The discrete-time Markov chain for DDPM is \(x_i = \sqrt{1-\beta_i}x_{i-1} + \sqrt{\beta_i}z_{i-1}\). In the continuous-time generalization:</p> \[x(t+\Delta t) = \sqrt{1-\beta(t+\Delta t)\Delta t}x(t) + \sqrt{\beta(t+\Delta t)\Delta t}z(t) \approx x(t) - \frac{1}{2}\beta(t+\Delta t)\Delta t x(t) + \sqrt{\beta(t)\Delta t}z(t)\] <p>We combine \(\sqrt{\Delta t}z(t) = dw\). The continuous0time DDPM is then</p> \[dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)}dw\] <p>which is also called variance-preserving SDE (VP-SDE).</p> <p><br></p> <h3 id="2-marginal-mean-and-variance-from-sde">2. Marginal mean and variance from SDE</h3> <p>Now we will use the SDEs to derive \(m(t)\) and \(v(t)\) for the mean and variance of the perturbation kernel \(p_{0t}\)</p> <p>Given an SDE with affine continuous function $f$ and $g$,</p> \[dx = f(x, t)dt + g(x, t)dw\] <p>Let</p> \[f(x, t) = A(t)x(t) + a(t)\] \[g(x, t) = B(t)x(t) + b(t)\] <p>and</p> \[\mathbb{E}[x(t)] = m(t)\] \[\mathbb{Var}[x(t)] = v(t)\] <p>will satisfy the following ODEs with initial conditions:</p> \[m'(t) = A(t)m(t) + a(t); m(0) = m_0\] \[v'(t) = 2A(t)v(t) + b^2(t); v(0) = v_0\] <p><br></p> <h3 id="3-solving-variable-coefficient-odes">3. Solving variable coefficient ODEs</h3> <p>The above ODEs are variable coefficient ODEs and have general solution. The general ODE \(y'(t) = a(t)y(t) + b(t)\) has solution</p> \[y(t) = Ce^{A(t)} + e^{A(t)}\int e^{-A(t)}b(t)dt\] <p>where</p> \[A(t) = \int a(t)dt\] <p><br></p> <h3 id="4-deriving-perturbation-kernels-from-sde">4. Deriving perturbation kernels from SDE</h3> <p>The perturbation kernel \(p_{0t}\) for SDE here is Gaussian. We are after the mean \(m(t)\) and variance \(v(t)\) for the distribution of \(x(t)\) given initial data point \(x(0)\). Note that because \(x(0)\) is out data point and we treat its distribution as a delta function, or a super tight gaussian with mean \(x(0)\) and variance \(v(0)=0I\).</p> <p><br></p> <h3 id="ve-sde">VE-SDE</h3> \[dx = \sqrt{\frac{d\sigma^2(t)}{dt}}dw\] <p>Using the above notation, \(A(t) = a(t) = B(t) = 0\) and \(b(t) = \sqrt{d\sigma^2(t)/dt}\).</p> \[m'(t) = 0 \Rightarrow m(t) = c = x(0)\] \[v'(t) = b^2(t) = \frac{d\sigma^2(t)}{dt} \Rightarrow v(t) = \sigma^2(t) + c = \sigma^2(t)\] <p>Therefore,</p> \[p_{0t}(x(t) | x(0)) = \mathcal{N}\left(x(t); x(0), \sigma^2(t)I \right)\] <p><br></p> <h3 id="vp-sde">VP-SDE</h3> \[dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)}dw\] <p>\(a(t) = B(t) = 0\), \(A(t) = -\beta(t)/2\) and \(b(t) = \sqrt{\beta(t)}\). Plug in the ODEs:</p> \[m'(t) = -\frac{1}{2} \beta(t)m(t) \Rightarrow m(t) = Ce^{\int_0^t-\frac{1}{2}\beta(s)ds} = x(0)e^{\int_0^t-\frac{1}{2}\beta(s)ds}\] \[v'(t) = -\beta(t)v(t)+\beta(t)\] \[v(t) = Ce^{-\int\beta(t)dt} + e^{-\int\beta(t)dt}\int e^{\int\beta(t)dt}\beta(t)dt = Ce^{-\int\beta(t)dt} + 1; v(0)=0 \Rightarrow C=-1\] \[v(t) = 1 - e^{-\int\beta(t)dt}\] <p>Therefore,</p> \[p_{0t}(x(t) | x(0)) = \mathcal{N}\left(x(t); x(0)e^{\int_0^t-\frac{1}{2}\beta(s)ds}, (1 - e^{-\int_0^t\beta(s)ds})I \right)\] <p><br></p> <h3 id="sub-vp-sde">sub VP-SDE</h3> <p>In the score-based SDE model, the author introduces another SDE called sub-VP SDE</p> \[dx = -\frac{1}{2}\beta(t)dt + \sqrt{\beta(t)\left(1-e^{-2\int_0^t\beta(s)ds} \right)}dw\] <p>Now we only change the \(b(t)\) so \(m(t)\) remains the same as VP-SDE.</p> \[v'(t) = -\beta(t)v(t) + \beta(t)\left(1-e^{-2\int_0^t\beta(s)ds} \right)\] \[v(t) = Ce^{-\int\beta(t)dt} + e^{-\int\beta(t)dt}\int e^{\int\beta(t)dt}\beta(t)dt + e^{-\int\beta(t)dt}\int e^{-\int\beta(t)dt}\beta(t)dt\] <p>The first two terms are the same as before:</p> \[v(t) =Ce^{-\int_0^t\beta(s)ds} + 1 + e^{-\int_0^t\beta(s)ds}e^{-\int_0^t\beta(s)ds} = Ce^{-\int_0^t\beta(s)ds} + 1 + e^{-2\int_0^t\beta(s)ds}\] <p>With $v(0) = 0 \Rightarrow C = -2$</p> \[v(t) = -2e^{-\int_0^t\beta(s)ds} + 1 + e^{-2\int_0^t\beta(s)ds} = \left(1-e^{-\int_0^t\beta(s)ds}\right)^2\] <p>Therefore,</p> \[p_{0t}(x(t) | x(0)) = \mathcal{N}\left(x(t); x(0)e^{\int_0^t-\frac{1}{2}\beta(s)ds}, \left(1 - e^{-\int_0^t\beta(s)ds}\right)^2I \right)\] <p>Note that the variance of sub VP-SDE is bounded by (or always less than) the variance of VP-SDE.</p> \[\forall t &gt; 0; \left(1 - e^{-\int_0^t\beta(s)ds}\right)^2 \le 1 - e^{-\int_0^t\beta(s)ds}\] <p>These 3 SDEs appear in Eq.(29) of [1] and Table 1 of [2]. In the following code, we will use VE-SDE as an example.</p> <h3 id="5-notebook">5. Notebook</h3> <p>The original notebook is provided by the author: <a href="https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing" rel="external nofollow noopener" target="_blank">Google Colab</a>.</p> <p>Most of the cells contain similar code to the DDPM, especially the time-conditional UNet model. I’ll just pick the cells I did not grasp during the first pass. If I have time after work, I might implement all the training and sampling for the above 3 SDEs.</p> <p><br></p> <h4 id="cell-5">Cell #5</h4> <p>Cell #5 sets up the VE-SDE scheduling for the diffusion coefficient \(g(t)\) and standard deviation \(\sqrt{v(t)}\). Note that in VE-SDE \(f(x, t) = 0\) so the mean of \(x(t)\): \(m(t) = 0\).</p> <p>We set up the Stochastic Differential Equation (VE-SDE) as the following</p> \[dx = \sigma^t dw\] <p>where \(\sigma &gt; 1.0\) is the standard deviation by design and \(dw\) is the Wiener process.</p> <p>This setup is not unique. The marginal probability standard deviation can be customized. The marginal probability variance is then</p> \[v(t) = \int_0^t g(s)^2ds\] <p>One can try different type of SDEs. One can verify that if \(g(t) = \sigma^t\) then</p> \[v(t) = \frac{\sigma^{2t} - 1}{2\log{\sigma}}\] <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Set up the VE-SDE: dx = sigma^t dw
</span>
<span class="k">def</span> <span class="nf">diffusion_coeff</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute the diffusion coefficient of our SDE.

    Args:
        t: A vector of time steps.
        sigma: The $\sigma$ in our SDE.

    Returns:
        The vector of diffusion coefficients.
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">sigma</span><span class="o">**</span><span class="n">t</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">marginal_prob_std</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute the standard deviation of $p_{0t}(x(t) | x(0))$.

    Args:    
        t: A vector of time steps.
        sigma: The $\sigma$ in our SDE.  

    Returns:
        The standard deviation.
    </span><span class="sh">"""</span>    
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">clone</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="n">sigma</span><span class="o">**</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">sigma</span><span class="p">))</span>


<span class="n">sigma</span> <span class="o">=</span> <span class="mf">25.</span>
<span class="n">marginal_prob_std_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span><span class="n">marginal_prob_std</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span>
<span class="n">diffusion_coeff_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span><span class="n">diffusion_coeff</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">)</span></code></pre></figure> <p><br></p> <h4 id="cell-6">Cell #6</h4> <p>Cell #6 sets up the loss function for the training objective. Recall that the score-function \(s_\theta(x, t)\) has to match \(\nabla_{x(t)}\log p_t(x(t))\) at everytime for every training data \(x(0)\). The DSM loss is then the regression loss.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Loss function: similar to the MSE loss 3
</span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">marginal_prob_std</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">The loss function for training score-based generative models.

    Args:
        model: A PyTorch model instance that represents a time-dependent score-based model.
        x: A mini-batch of training data.    
        marginal_prob_std: A function that gives the standard deviation of the perturbation kernel.
        eps: A tolerance value for numerical stability.
    </span><span class="sh">"""</span>
    <span class="c1"># x is the original signal, without purturbation or noising
</span>
    <span class="c1"># uniformly sample random_t in [0, 1] using the batch dimension
</span>    <span class="n">random_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span> 

    <span class="c1"># Random Gaussian Noises
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 

    <span class="c1"># Compute the std at these time points
</span>    <span class="n">std</span> <span class="o">=</span> <span class="nf">marginal_prob_std</span><span class="p">(</span><span class="n">random_t</span><span class="p">)</span> 

    <span class="c1"># forward computation p(x(t) | x(0)) = N(x(0), v(t))
</span>    <span class="n">perturbed_x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">*</span> <span class="n">std</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> 

    <span class="c1"># compute the score 
</span>    <span class="n">score</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">perturbed_x</span><span class="p">,</span> <span class="n">random_t</span><span class="p">)</span> 

    <span class="c1"># true_score = - (x(t) - x(0)) / (std_t ^ 2)
</span>    <span class="c1">#            = - z_t / std_t
</span>    <span class="c1"># L_dsm = (pred_score - true_score) ^ 2 = (pred_score + z_t / std_t) ^ 2
</span>    <span class="c1"># scaled_L_dsm = (pred_score * std_t + z_t) ^ 2
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">score</span> <span class="o">*</span> <span class="n">std</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">+</span> <span class="n">z</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)))</span> 
    <span class="k">return</span> <span class="n">loss</span></code></pre></figure> <p><br></p> <h4 id="cell-8">Cell #8</h4> <p>Cell #8 prepares sampling with 3 different methods: Euler-Maruyama, Predictor-Corrector and ODE.</p> <h5 id="euler-maruyama">Euler-Maruyama</h5> <p>Recall that SDE of the form</p> \[dx = f(x, t)dt + g(t)dw\] <p>has the reverse-time SDE:</p> \[dx = \left[f(x, t) - g^2(t)\nabla_{x(t)}\log p_t(x(t)) \right]dt + g(t)dw\] \[dx = -\sigma^{2t} s_\theta(x, t) dt + \sigma^t dw; dt &lt; 0\] \[x_{t-\Delta t} = \mathbf{x}_t + \sigma^{2t} s_\theta(x_t, t)\Delta t + \sigma^t\sqrt{\Delta t} z_t\] <p>where \(z_t \sim \mathcal{N}(0, I)\).</p> <p>Euler-Maruyama applies \(dt \sim \Delta t\) discretization.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">num_steps</span> <span class="o">=</span> <span class="mi">500</span>
<span class="k">def</span> <span class="nf">Euler_Maruyama_sampler</span><span class="p">(</span><span class="n">score_model</span><span class="p">,</span> 
                           <span class="n">marginal_prob_std</span><span class="p">,</span>
                           <span class="n">diffusion_coeff</span><span class="p">,</span> 
                           <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
                           <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span> 
                           <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span> 
                           <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Generate samples from score-based models with the Euler-Maruyama solver.

    Args:
        score_model: A PyTorch model that represents the time-dependent score-based model.
        marginal_prob_std: A function that gives the standard deviation of the perturbation kernel.
        diffusion_coeff: A function that gives the diffusion coefficient of the SDE.
        batch_size: The number of samplers to generate by calling this function once.
        num_steps: The number of sampling steps. Equivalent to the number of discretized time steps.
        device: </span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="s"> for running on GPUs, and </span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="s"> for running on CPUs.
        eps: The smallest time step for numerical stability.
    
    Returns:
        Samples.
    </span><span class="sh">"""</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">init_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="nf">marginal_prob_std</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">time_steps</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">time_steps</span><span class="p">):</span>      
            <span class="n">batch_time_step</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">time_step</span>
            <span class="n">g</span> <span class="o">=</span> <span class="nf">diffusion_coeff</span><span class="p">(</span><span class="n">batch_time_step</span><span class="p">)</span>
            
            <span class="c1"># mean_x = x + g^2(t) s(x, t) dt
</span>            <span class="n">mean_x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">g</span><span class="o">**</span><span class="mi">2</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="nf">score_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_time_step</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span> 
            
            <span class="c1"># x = mean_x + g(t) \sqrt{dt} z; z ~ N(0, I)
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">mean_x</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">step_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
    
    <span class="c1"># Do not include any noise in the last sampling step.
</span>    <span class="k">return</span> <span class="n">mean_x</span></code></pre></figure> <p><br></p> <h5 id="predictor-corrector">Predictor-Corrector</h5> <p>Recall that given the score function \(s_\theta(x, t)\), we can sample via Langevin dynamics:</p> \[x_{i+1} = x_i + \epsilon \nabla_{x_i} \log p(x_i) + \sqrt{2\epsilon} z_i\] <p>The PC sampling combines the ODE/SDE solver (Predictor) with $N$ steps of local Langevin dynamics (Correcor).</p> <ol> <li>Predictor: Use ODE/SDE solver for the next time step $x(t-dt)$ using $s(x(t), t)$</li> <li>Corrector: Still using the score $s(x(t-dt), t-dt)$ and Langevin dynamics to correct for $x(t-dt)$ for $N$ steps</li> </ol> <p>The step size \(\epsilon\) is determined with predefined \(r\):</p> \[\epsilon = 2 \left(r \frac{\|z\|_2}{\|\nabla_{x} \log p(x)\|_2}\right)^2\] <p>which is determined by the norm of the score. The idea behind is like the adaptive step size in the stochastic gradient descent where we want to take a smaller (more careful) step when the score/gradient (slope) is steep to avoid overshoot (or sliding).</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">signal_to_noise_ratio</span> <span class="o">=</span> <span class="mf">0.16</span> 

<span class="c1">## The number of sampling steps.
</span><span class="n">num_steps</span> <span class="o">=</span>  <span class="mi">500</span>
<span class="k">def</span> <span class="nf">pc_sampler</span><span class="p">(</span><span class="n">score_model</span><span class="p">,</span> 
               <span class="n">marginal_prob_std</span><span class="p">,</span>
               <span class="n">diffusion_coeff</span><span class="p">,</span>
               <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
               <span class="n">num_steps</span><span class="o">=</span><span class="n">num_steps</span><span class="p">,</span> 
               <span class="n">snr</span><span class="o">=</span><span class="n">signal_to_noise_ratio</span><span class="p">,</span>                
               <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span>
               <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Generate samples from score-based models with Predictor-Corrector method.

    Args:
    score_model: A PyTorch model that represents the time-dependent score-based model.
    marginal_prob_std: A function that gives the standard deviation
      of the perturbation kernel.
    diffusion_coeff: A function that gives the diffusion coefficient 
      of the SDE.
    batch_size: The number of samplers to generate by calling this function once.
    num_steps: The number of sampling steps. 
      Equivalent to the number of discretized time steps.    
    device: </span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="s"> for running on GPUs, and </span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="s"> for running on CPUs.
    eps: The smallest time step for numerical stability.

    Returns: 
    Samples.
    </span><span class="sh">"""</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">init_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="nf">marginal_prob_std</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">time_steps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
    <span class="n">step_size</span> <span class="o">=</span> <span class="n">time_steps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">time_steps</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">time_step</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">time_steps</span><span class="p">):</span>      
            <span class="n">batch_time_step</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">time_step</span>
            
            
            <span class="c1"># N = 1, Corrector step (Langevin MCMC)
</span>            <span class="c1"># for N &gt; 1, just wrap this part in a loop, but it will be expensive
</span>            <span class="n">grad</span> <span class="o">=</span> <span class="nf">score_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_time_step</span><span class="p">)</span>
            <span class="n">grad_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">grad</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">mean</span><span class="p">()</span>
            <span class="n">noise_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>

            <span class="c1"># eps = 2 (r |z| / |g|) ^ 2
</span>            <span class="n">langevin_step_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">snr</span> <span class="o">*</span> <span class="n">noise_norm</span> <span class="o">/</span> <span class="n">grad_norm</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> 
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">langevin_step_size</span> <span class="o">*</span> <span class="n">grad</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">langevin_step_size</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


            <span class="c1"># Predictor step (Euler-Maruyama)
</span>            <span class="n">g</span> <span class="o">=</span> <span class="nf">diffusion_coeff</span><span class="p">(</span><span class="n">batch_time_step</span><span class="p">)</span>
            <span class="n">x_mean</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">g</span><span class="o">**</span><span class="mi">2</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="nf">score_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch_time_step</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x_mean</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">g</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">step_size</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      

    <span class="c1"># The last step does not include any noise
</span>    <span class="k">return</span> <span class="n">x_mean</span></code></pre></figure> <p><br></p> <h5 id="probability-flow-ode">Probability flow ODE</h5> <p>For probability flow ODE, the reverse process:</p> \[dx = \left[f(x, t) - \frac{1}{2}g^2(t)\nabla_{x(t)}\log p_t(x(t)) \right]dt\] \[dx = -\frac{1}{2}\sigma^{2t} s_\theta(x, t) dt\] \[\frac{dx}{dt} = -\frac{1}{2}\sigma^{2t} s_\theta(x, t)\] <p>Now we need to integrate from \(t=T\) to \(t=0\)</p> \[x(t) = \int_T^t\frac{dx}{dt} dt + x(T) = \int_T^t -\frac{1}{2}\sigma^{2t} s_\theta(x, t) dt + x(T)\] \[x(0) = \int_T^0 -\frac{1}{2}\sigma^{2t} s_\theta(x, t) dt + x(T)\] <p>the above can be solved using existent ODE solver, such as Runge-Kutta.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">scipy</span> <span class="kn">import</span> <span class="n">integrate</span>

<span class="c1">## The error tolerance for the black-box ODE solver
</span><span class="n">error_tolerance</span> <span class="o">=</span> <span class="mf">1e-5</span> 
<span class="k">def</span> <span class="nf">ode_sampler</span><span class="p">(</span><span class="n">score_model</span><span class="p">,</span>
                <span class="n">marginal_prob_std</span><span class="p">,</span>
                <span class="n">diffusion_coeff</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
                <span class="n">atol</span><span class="o">=</span><span class="n">error_tolerance</span><span class="p">,</span> 
                <span class="n">rtol</span><span class="o">=</span><span class="n">error_tolerance</span><span class="p">,</span> 
                <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span> 
                <span class="n">z</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Generate samples from score-based models with black-box ODE solvers.

    Args:
    score_model: A PyTorch model that represents the time-dependent score-based model.
    marginal_prob_std: A function that returns the standard deviation 
      of the perturbation kernel.
    diffusion_coeff: A function that returns the diffusion coefficient of the SDE.
    batch_size: The number of samplers to generate by calling this function once.
    atol: Tolerance of absolute errors.
    rtol: Tolerance of relative errors.
    device: </span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="s"> for running on GPUs, and </span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="s"> for running on CPUs.
    z: The latent code that governs the final sample. If None, we start from p_1;
      otherwise, we start from the given z.
    eps: The smallest time step for numerical stability.
    </span><span class="sh">"""</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># Create the latent code
</span>    <span class="k">if</span> <span class="n">z</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">init_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="nf">marginal_prob_std</span><span class="p">(</span><span class="n">t</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">init_x</span> <span class="o">=</span> <span class="n">z</span>

    <span class="n">shape</span> <span class="o">=</span> <span class="n">init_x</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">score_eval_wrapper</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">A wrapper of the score-based model for use by the ODE solver.</span><span class="sh">"""</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">))</span>    
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> 
            <span class="n">score</span> <span class="o">=</span> <span class="nf">score_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">score</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,)).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ode_func</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>        
        <span class="sh">"""</span><span class="s">The ODE function for use by the ODE solver.
           dx = - 0.5 * g^2(x) * s(x, t) dt
        </span><span class="sh">"""</span>
        <span class="n">time_steps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span> <span class="o">*</span> <span class="n">t</span>
        <span class="n">g</span> <span class="o">=</span> <span class="nf">diffusion_coeff</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="k">return</span>  <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">g</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="nf">score_eval_wrapper</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">)</span>

    <span class="c1"># Run the black-box ODE solver.
</span>    <span class="c1"># solving x' = dx / dt = - 0.5 * g^2(x) * s(x, t); x(0) = x_0 = init_x
</span>    <span class="c1"># note solving from t = 1 to t = 0
</span>    <span class="n">res</span> <span class="o">=</span> <span class="n">integrate</span><span class="p">.</span><span class="nf">solve_ivp</span><span class="p">(</span><span class="n">ode_func</span><span class="p">,</span> <span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">eps</span><span class="p">),</span> <span class="n">init_x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">rtol</span><span class="o">=</span><span class="n">rtol</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">atol</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">RK45</span><span class="sh">'</span><span class="p">)</span>  
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of function evaluations: </span><span class="si">{</span><span class="n">res</span><span class="p">.</span><span class="n">nfev</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">y</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span></code></pre></figure> <p><br></p> <p>The results of the above sampling methods are shown below. From left to right are: Euler-Maruyama, Predictor-Corrector and Probability flow ODE.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/sde/em-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/sde/em-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/sde/em-1400.webp"></source> <img src="/assets/img/posts/sde/em.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/sde/pc-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/sde/pc-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/sde/pc-1400.webp"></source> <img src="/assets/img/posts/sde/pc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/sde/ode-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/sde/ode-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/sde/ode-1400.webp"></source> <img src="/assets/img/posts/sde/ode.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><br></p> <h3 id="6-likelihood-computation">6. Likelihood Computation</h3> <p>We can compute the likelihood \(\log p_0(x(0))\) using</p> \[\log p_0(x(0)) = \log p_T(x(T)) + \int_0^T \nabla \cdot \left[ -\frac{1}{2}\sigma^{2t} s_\theta(x, t) \right] dt\] <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># likelihood
</span>
<span class="c1">#@title Define the likelihood function (double click to expand or collapse)
</span>
<span class="k">def</span> <span class="nf">prior_likelihood</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">The likelihood of a Gaussian distribution with mean zero and 
      standard deviation sigma.</span><span class="sh">"""</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">N</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">z</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ode_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> 
                   <span class="n">score_model</span><span class="p">,</span>
                   <span class="n">marginal_prob_std</span><span class="p">,</span> 
                   <span class="n">diffusion_coeff</span><span class="p">,</span>
                   <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> 
                   <span class="n">device</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span>
                   <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute the likelihood with probability flow ODE.

    Args:
    x: Input data.
    score_model: A PyTorch model representing the score-based model.
    marginal_prob_std: A function that gives the standard deviation of the 
      perturbation kernel.
    diffusion_coeff: A function that gives the diffusion coefficient of the 
      forward SDE.
    batch_size: The batch size. Equals to the leading dimension of `x`.
    device: </span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="s"> for evaluation on GPUs, and </span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="s"> for evaluation on CPUs.
    eps: A `float` number. The smallest time step for numerical stability.

    Returns:
    z: The latent code for `x`.
    bpd: The log-likelihoods in bits/dim.
    </span><span class="sh">"""</span>

    <span class="c1"># Draw the random Gaussian sample for Skilling-Hutchinson's estimator.
</span>    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      
    <span class="k">def</span> <span class="nf">divergence_eval</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>      
        <span class="sh">"""</span><span class="s">Compute the divergence of the score-based model with Skilling-Hutchinson.</span><span class="sh">"""</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">enable_grad</span><span class="p">():</span>
            <span class="n">sample</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">score_e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="nf">score_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span>
            <span class="n">grad_score_e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">score_e</span><span class="p">,</span> <span class="n">sample</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">grad_score_e</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>    

    <span class="n">shape</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">def</span> <span class="nf">score_eval_wrapper</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">A wrapper for evaluating the score-based model for the black-box ODE solver.</span><span class="sh">"""</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">))</span>    
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>    
            <span class="n">score</span> <span class="o">=</span> <span class="nf">score_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">score</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,)).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">divergence_eval_wrapper</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">A wrapper for evaluating the divergence of score for the black-box ODE solver.</span><span class="sh">"""</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="c1"># Obtain x(t) by solving the probability flow ODE.
</span>            <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">time_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">time_steps</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">).</span><span class="nf">reshape</span><span class="p">((</span><span class="n">sample</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">))</span>    
            <span class="c1"># Compute likelihood.
</span>            <span class="n">div</span> <span class="o">=</span> <span class="nf">divergence_eval</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">div</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,)).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ode_func</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">The ODE function for the black-box solver.</span><span class="sh">"""</span>
        <span class="n">time_steps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span> <span class="o">*</span> <span class="n">t</span>    
        <span class="n">sample</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">logp</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:]</span>
        <span class="n">g</span> <span class="o">=</span> <span class="nf">diffusion_coeff</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
        <span class="n">sample_grad</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">g</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="nf">score_eval_wrapper</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">)</span> <span class="c1"># dx / dt
</span>        <span class="n">logp_grad</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">g</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="nf">divergence_eval_wrapper</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">time_steps</span><span class="p">)</span> <span class="c1"># d logp / dt
</span>        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">sample_grad</span><span class="p">,</span> <span class="n">logp_grad</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># d[x, logp] / dt
</span>
    <span class="c1"># logp_0(x(0)) = logp_T(x(T)) + int_0^T \nabla f(x(t), t)
</span>    <span class="n">init</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">x</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># Black-box ODE solver
</span>    <span class="c1"># note solving from t = 0 to t = 1, different from the reverse one
</span>    <span class="n">res</span> <span class="o">=</span> <span class="n">integrate</span><span class="p">.</span><span class="nf">solve_ivp</span><span class="p">(</span><span class="n">ode_func</span><span class="p">,</span> <span class="p">(</span><span class="n">eps</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">init</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">RK45</span><span class="sh">'</span><span class="p">)</span>  
    
    <span class="n">zp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">y</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># [x(1), logp(1)]
</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">zp</span><span class="p">[:</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]].</span><span class="nf">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># x(1)
</span>    <span class="n">delta_logp</span> <span class="o">=</span> <span class="n">zp</span><span class="p">[</span><span class="o">-</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:].</span><span class="nf">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># logp(1)
</span>    <span class="n">sigma_max</span> <span class="o">=</span> <span class="nf">marginal_prob_std</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">]))</span>
    <span class="c1"># print(sigma_max)
</span>    
    <span class="c1"># compute likelihood of x(T)
</span>    <span class="n">prior_logp</span> <span class="o">=</span> <span class="nf">prior_likelihood</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">sigma_max</span><span class="p">)</span> 
    
    <span class="c1"># bits per dimension
</span>    <span class="n">bpd</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">prior_logp</span> <span class="o">+</span> <span class="n">delta_logp</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># log_2 (x) = log(x) / log(2)
</span>    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

    <span class="c1"># we treat the gray scale [0, 1.] as [0., 256], so we have to add log_2(256) = 8 to each dimension
</span>    <span class="c1"># or according to the def: bpd = -(prior_logp + delta_logp - log(256)) / log(2)
</span>    <span class="c1">#                              = -(prior_logp + delta_logp) / log(2) + 8.
</span>    <span class="n">bpd</span> <span class="o">=</span> <span class="n">bpd</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="mf">8.</span> 
    
    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">bpd</span></code></pre></figure> <p>We can use the above to compute the likelihood as bits per dimension, the lower the better.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nc">MNIST</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">data_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>


<span class="n">all_bpds</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">all_items</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">try</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span> <span class="k">break</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="c1"># uniform dequantization
</span>        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="mf">255.</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="o">/</span> <span class="mf">256.</span>    
        <span class="n">_</span><span class="p">,</span> <span class="n">bpd</span> <span class="o">=</span> <span class="nf">ode_likelihood</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">score_model</span><span class="p">,</span> <span class="n">marginal_prob_std_fn</span><span class="p">,</span>
                                <span class="n">diffusion_coeff_fn</span><span class="p">,</span>
                                <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
        <span class="n">all_bpds</span> <span class="o">+=</span> <span class="n">bpd</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
        <span class="n">all_items</span> <span class="o">+=</span> <span class="n">bpd</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Average bits/dim: {:5f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">all_bpds</span> <span class="o">/</span> <span class="n">all_items</span><span class="p">))</span>

<span class="k">except</span> <span class="nb">KeyboardInterrupt</span><span class="p">:</span> <span class="k">raise</span>

<span class="c1"># Average bits/dim: 1.710447
# Average bits/dim: 1.658721</span></code></pre></figure> <p><br></p> <h3 id="7-references">7. References</h3> <ol> <li>Song et al, Score-Based Generative Modeling through Stochastic Differential Equations, (<a href="https://openreview.net/forum?id=PxTIG12RRHS" rel="external nofollow noopener" target="_blank">link</a>)</li> <li>Song et al, Maximum Likelihood Training of Score-Based Diffusion Models, (<a href="https://arxiv.org/abs/2101.09258" rel="external nofollow noopener" target="_blank">link</a>)</li> <li>Original PyTorch Implementation: <a href="https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing" rel="external nofollow noopener" target="_blank">Google Colab</a> </li> <li>Blog post on Score-Based SDE <a href="https://yang-song.net/blog/2021/score/" rel="external nofollow noopener" target="_blank">link</a> </li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yen-Lin Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>