<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Playing with DDPM | Yen-Lin Chen</title> <meta name="author" content="Yen-Lin Chen"> <meta name="description" content="Playing with Ho et al. " denoising diffusion probabilistic model> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jipq6175.github.io/blog/2023/diffusion/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yen-Lin </span>Chen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Playing with DDPM</h1> <p class="post-meta">February 8, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/generating"> <i class="fas fa-hashtag fa-sm"></i> generating</a>   <a href="/blog/tag/coding"> <i class="fas fa-hashtag fa-sm"></i> coding</a>   <a href="/blog/tag/reading"> <i class="fas fa-hashtag fa-sm"></i> reading</a>     ·   <a href="/blog/category/models"> <i class="fas fa-tag fa-sm"></i> models</a>   </p> </header> <article class="post-content"> <p>The denoising diffusion probabilistic model (DDPM) is the model behind <a href="https://stablediffusionweb.com/" rel="external nofollow noopener" target="_blank">Stable Diffusion</a>, <a href="https://openai.com/product/dall-e-2" rel="external nofollow noopener" target="_blank">DALL-E 2</a>, <a href="https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F" rel="external nofollow noopener" target="_blank">Midjourney</a> and <a href="https://imagen.research.google/" rel="external nofollow noopener" target="_blank">Imagen</a>. It has applications in <a href="https://imagen.research.google/video/" rel="external nofollow noopener" target="_blank">video generation</a> <a href="https://arxiv.org/abs/2203.17003" rel="external nofollow noopener" target="_blank">molecular generation</a>, <a href="https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1" rel="external nofollow noopener" target="_blank">protein generation</a> and <a href="https://arxiv.org/abs/2211.14169" rel="external nofollow noopener" target="_blank">superior latent space sampling</a>.</p> <p>I planned to explore the idea and math behind DDPM and play with some coding and generative image models. I was following <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html" rel="external nofollow noopener" target="_blank">the original DDPM paper</a>, <a href="https://arxiv.org/abs/2208.11970" rel="external nofollow noopener" target="_blank">this nice paper</a> and <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">this blog</a> for the mathematical formulations and <a href="https://huggingface.co/blog/annotated-diffusion" rel="external nofollow noopener" target="_blank">the annotated diffusion model</a> for the implementations.</p> <hr> <h3 id="the-original-diffusion-model">The Original Diffusion Model</h3> <ul> <li>The variance schedule can be improved from <code class="language-plaintext highlighter-rouge">linear</code> to <code class="language-plaintext highlighter-rouge">cosine</code> </li> <li>The model can be improved by learning the variances of \(p_\theta(x_{t-1} | x_t)\) instead of fixing it</li> <li>The loss \(L = L_{simple} + \lambda L_{vlb}\)</li> </ul> <p>The forward process is \(p(x_t | x_{t-1})\) and the reverse denoising process is \(q(x_{t-1} | x_t)\). A good close-form property is that we don’t need to apply \(q\) iteratively to sample \(x_t\). We have</p> \[q(x_t|x_0) = N(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I)\] \[\alpha_t = 1 - \beta_t\] \[\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\] <p>This allows us to optimize random terms of the loss \(L\). I.e. randomly sample \(t\) during the training to optimize \(L_t\)</p> <p>Shown in the Ho et al 2020, one can reparametrize the mean to make the network learn the added noise \(\epsilon_\theta(x_t, t)\) for noise level \(t\) in the KL terms which constitute the losses. This means that our NN becomes a noise predictor, rather than a direct mean predictor. The mean can be computed as</p> \[\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}\left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}\epsilon_\theta(x_t, t) \right)\] <p>The final objective function \(L_t\) then looks as the following, for a random time step \(t\) given \(\epsilon \sim N(0, I)\):</p> \[|| \epsilon - \epsilon_\theta(x_t, t) ||^2 = || \epsilon - \epsilon_\theta\left( \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon, t \right)||^2\] <p>So:</p> <ul> <li>We take a random sample \(x_0\) from the real unknown and possibly complex distribution \(q(x_0)\)</li> <li>We sample a noise level \(t\) uniformly (or complex) between \(1\) and \(T\) (i.e. random time step)</li> <li>We sample some noise from a Gaussian distribution and corupt the input by this noise level \(t\) using above equations</li> <li>The neural network is trained to predict this noise based on the corrupted image \(x_t\), i.e. noise applied on \(x_0\) based on known schedule \(\beta_t\)</li> <li>In practice, this is done on batches of data as one uses SGD to optimize the neural network</li> </ul> <hr> <h3 id="0-dependencies">0. Dependencies</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># python: 3.6.9
</span>
<span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">torch</span><span class="p">,</span> <span class="n">torchvision</span><span class="p">,</span> <span class="n">math</span><span class="p">,</span> <span class="n">requests</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="n">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="n">inspect</span> <span class="kn">import</span> <span class="n">isfunction</span>
<span class="kn">from</span> <span class="n">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="n">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="n">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>
<span class="kn">from</span> <span class="n">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="n">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="n">torchvision.utils</span> <span class="kn">import</span> <span class="n">save_image</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>


<span class="c1"># utilities functions
</span><span class="k">def</span> <span class="nf">exists</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">default</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span> 
    <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">val</span><span class="p">):</span> <span class="k">return</span> <span class="n">val</span>
    <span class="k">return</span> <span class="nf">d</span><span class="p">()</span> <span class="k">if</span> <span class="nf">isfunction</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">else</span> <span class="n">d</span></code></pre></figure> <hr> <h3 id="1-model-architecture">1. Model Architecture</h3> <p>The model’s input and output have the same dimension. It uses the noisy image at \(t\) to predict the corresponding noise and then remove that noise based on variance scheduling to obtain slightly denoised image at \(t-1\) and so on until fully denoised at \(t=1\). The trick to make the model <code class="language-plaintext highlighter-rouge">aware</code> of what kind of filter (lowpass or highpass or something in between) it should behave lies in the time step embedding.</p> <p>The model consists of 3 parts: <code class="language-plaintext highlighter-rouge">UNet</code>, <code class="language-plaintext highlighter-rouge">Time Embedding</code> and pixel-pixel <code class="language-plaintext highlighter-rouge">Attention</code>.</p> <h4 id="11-blocks-for-unet">1.1 Blocks for UNet</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Helper classes
</span><span class="k">class</span> <span class="nc">Residual</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">Residual</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> 
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>

<span class="c1"># up and down sampling from convolution layers
</span><span class="k">def</span> <span class="nf">Upsample</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">Downsample</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span> <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1"># Upsampling
</span><span class="nf">print</span><span class="p">(</span><span class="nc">Upsample</span><span class="p">(</span><span class="mi">32</span><span class="p">))</span>

<span class="c1"># Downsampling
</span><span class="nf">print</span><span class="p">(</span><span class="nc">Downsample</span><span class="p">(</span><span class="mi">32</span><span class="p">))</span></code></pre></figure> <p>Outputs:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nc">Conv2d</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span></code></pre></figure> <p>The UNet blocks using <code class="language-plaintext highlighter-rouge">ResNet</code> or <code class="language-plaintext highlighter-rouge">ConvNeXT</code> convolutional models:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># GroupNorm
</span><span class="k">class</span> <span class="nc">GNorm</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">GNorm</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">fn</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Block Layer
</span><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">Block</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

        <span class="c1"># projection, normalize and activation
</span>        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale_shift</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span> 
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">scale_shift</span><span class="p">):</span> 
            <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span> <span class="o">=</span> <span class="n">scale_shift</span>
            <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>
        
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># ResNet: Noise2Noise
</span><span class="k">class</span> <span class="nc">ResNetBlock</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ResNetBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_emb_dim</span> <span class="o">=</span> <span class="n">time_emb_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span> 
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">))</span> <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">)</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="n">self</span><span class="p">.</span><span class="n">block1</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">block2</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span><span class="n">output_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="n">output_dim</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">exists</span><span class="p">(</span><span class="n">time_emb</span><span class="p">):</span>
            <span class="n">time_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">time_emb</span><span class="p">,</span> <span class="sh">'</span><span class="s">b c -&gt; b c 1 1</span><span class="sh">'</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span>
        
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">block2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span>


<span class="c1"># ConvNeXT: Noise2Noise
</span>
<span class="k">class</span> <span class="nc">ConvNextBlock</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">ConvNextBlock</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_emb_dim</span> <span class="o">=</span> <span class="n">time_emb_dim</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span> <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">)</span> <span class="k">else</span> <span class="bp">None</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">ds_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">norm</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span> 
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span> <span class="o">*</span> <span class="n">mult</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_dim</span> <span class="o">*</span> <span class="n">mult</span><span class="p">),</span> 
                                       <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">output_dim</span> <span class="o">*</span> <span class="n">mult</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="n">output_dim</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">()</span>
        
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span> 
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ds_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">exists</span><span class="p">(</span><span class="n">time_emb</span><span class="p">):</span> 
            <span class="n">time_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">time_emb</span><span class="p">,</span> <span class="sh">'</span><span class="s">b c -&gt; b c 1 1</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">net</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="c1"># print(h.shape)
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span>


<span class="c1"># testing the conv blocks
# batch x channel x H x W
</span><span class="n">bs</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">batch_image</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">((</span><span class="n">bs</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Batch Image = </span><span class="sh">'</span><span class="p">,</span> <span class="n">batch_image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">output_dim</span> <span class="o">=</span> <span class="mi">96</span>

<span class="n">TSE</span> <span class="o">=</span> <span class="nc">TimeStepEmbedding</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">batch_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">))</span>
<span class="n">tse</span> <span class="o">=</span> <span class="nc">TSE</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Time Emb = </span><span class="sh">'</span><span class="p">,</span> <span class="n">tse</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># testing Block
</span><span class="n">block</span> <span class="o">=</span> <span class="nc">Block</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>
<span class="n">block_out</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">batch_image</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Block output = </span><span class="sh">'</span><span class="p">,</span> <span class="n">block_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># testing ResNetBlock
</span><span class="n">resnetblock</span> <span class="o">=</span> <span class="nc">ResNetBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">resnetblock_out</span> <span class="o">=</span> <span class="nf">resnetblock</span><span class="p">(</span><span class="n">batch_image</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="n">tse</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">ResnetBlock output = </span><span class="sh">'</span><span class="p">,</span> <span class="n">resnetblock_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># testing ConvNextBlock
</span><span class="n">convnectblock</span> <span class="o">=</span> <span class="nc">ConvNextBlock</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">convnectblock_out</span> <span class="o">=</span> <span class="nf">convnectblock</span><span class="p">(</span><span class="n">batch_image</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="n">tse</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">ConvnextBlock output = </span><span class="sh">'</span><span class="p">,</span> <span class="n">convnectblock_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Batch</span> <span class="n">Image</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span>
<span class="n">Time</span> <span class="n">Emb</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
<span class="n">Block</span> <span class="n">output</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span>
<span class="n">ResnetBlock</span> <span class="n">output</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span>
<span class="n">ConvnextBlock</span> <span class="n">output</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span></code></pre></figure> <h4 id="12-time-embeddings">1.2 Time Embeddings</h4> <p>The time embeddings are from the sequence positional encoding of the <a href="https://arxiv.org/abs/1706.03762" rel="external nofollow noopener" target="_blank">Transformer paper</a>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">TimeStepEmbedding</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span> 
        <span class="nf">super</span><span class="p">(</span><span class="n">TimeStepEmbedding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">Dimension has to be even number</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">time</span><span class="p">):</span> 
        <span class="c1"># time is a tensor
</span>        <span class="n">device</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">device</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">time</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">embeddings</span><span class="p">.</span><span class="nf">sin</span><span class="p">(),</span> <span class="n">embeddings</span><span class="p">.</span><span class="nf">cos</span><span class="p">()],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span>


<span class="c1"># test time embeddings
</span><span class="n">TSE</span> <span class="o">=</span> <span class="nc">TimeStepEmbedding</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>
<span class="n">batch_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">batched time:</span><span class="sh">'</span><span class="p">,</span> <span class="n">batch_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">batch_tse</span> <span class="o">=</span> <span class="nc">TSE</span><span class="p">(</span><span class="n">batch_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">batched time embeddings:</span><span class="sh">'</span><span class="p">,</span> <span class="n">batch_tse</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">batched</span> <span class="n">time</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="n">batched</span> <span class="n">time</span> <span class="n">embeddings</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span></code></pre></figure> <h4 id="13-attention">1.3 Attention</h4> <p>2 variants of attention:</p> <ul> <li>regular multi-head self-attention (as used in the Transformer),</li> <li>linear attention variant (Shen et al., 2018), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.</li> </ul> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Original Attention (in the Conv Net Context)
</span><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span> 

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim_head</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>

        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">to_qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># compute qkv at the same timne
</span>        <span class="n">self</span><span class="p">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># batch, channel, height, width
</span>        
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (q, k, v) tuple
</span>        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="sh">'</span><span class="s">b (h c) x y -&gt; b h c (x y)</span><span class="sh">'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">qkv</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>

        <span class="n">sim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">b h d i, b h d j -&gt; b h i j</span><span class="sh">'</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">sim</span> <span class="o">-</span> <span class="n">sim</span><span class="p">.</span><span class="nf">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># batch x heads x (h x w) x (h x w)
</span>        <span class="c1"># attention is pixel to pixel weight
</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">b h i j, b h d j -&gt; b h i d</span><span class="sh">'</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="sh">'</span><span class="s">b h (x y) d -&gt; b (h d) x y</span><span class="sh">'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="c1"># Linear Attention
</span><span class="k">class</span> <span class="nc">LinearAttention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim_head</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">LinearAttention</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>
        <span class="n">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>

        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">to_qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                                          <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="sh">'</span><span class="s">b (h c) x y -&gt; b h c (x y)</span><span class="sh">'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">qkv</span><span class="p">)</span>
        
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">b h d n, b h e n -&gt; b h d e</span><span class="sh">'</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">b h d e, b h d n -&gt; b h e n</span><span class="sh">'</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="sh">'</span><span class="s">b h c (x y) -&gt; b (h c) x y</span><span class="sh">'</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">to_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># Test Attention
</span><span class="n">attn</span> <span class="o">=</span> <span class="nc">Attention</span><span class="p">(</span><span class="mi">96</span><span class="p">)</span>
<span class="n">attn_out</span> <span class="o">=</span> <span class="nf">attn</span><span class="p">(</span><span class="n">block_out</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Attention out = </span><span class="sh">'</span><span class="p">,</span> <span class="n">attn_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>


<span class="c1"># Test Linear Attention
</span><span class="n">lattn</span> <span class="o">=</span> <span class="nc">LinearAttention</span><span class="p">(</span><span class="mi">96</span><span class="p">)</span>
<span class="n">lattn_out</span> <span class="o">=</span> <span class="nf">lattn</span><span class="p">(</span><span class="n">block_out</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Linear Attention out = </span><span class="sh">'</span><span class="p">,</span> <span class="n">lattn_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Attention</span> <span class="n">out</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span>
<span class="n">Linear</span> <span class="n">Attention</span> <span class="n">out</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span></code></pre></figure> <h4 id="14-the-unet">1.4 The UNet</h4> <p>The job of the network $\epsilon_\theta(x_t, t)$ is to take in a batch of noisy images + noisy levels and output the noise added to the input. I.e. the network takes a batch of noisy images of shape <code class="language-plaintext highlighter-rouge">(batch_size, num_channels, height, width)</code> and batch of noise levels <code class="language-plaintext highlighter-rouge">(batch_size, 1)</code> as input. Note that noise levels are just <code class="language-plaintext highlighter-rouge">batch_size</code> random integers. And the network returns a tensor of the same shape <code class="language-plaintext highlighter-rouge">(batch_size, num_channels, height, width)</code> as the predicted noises.</p> <p>The network is built up as follows:</p> <ul> <li> <p>First, a conv layer is applied on the batch of noisy images, positional embeddings are computed from the noisy level (an integer).</p> </li> <li> <p>Second, a sequence of downsampling stahes are applkied. Each downsampling stage consists of</p> <ol> <li>2 ResNet/ConvNeXT blocks</li> <li>Group norm</li> <li>Attention</li> <li>Residual connection</li> <li>Down sampling operation</li> </ol> </li> <li> <p>At the middle of the network, ResNet/ConvNeXT blocks are applied, interleaved with attention</p> </li> <li> <p>Next, a sequence of upsampling stages are applied</p> <ol> <li>2 ResNet/ConvNeXT blocks</li> <li>Group norm</li> <li>Attention</li> <li>Residual connection</li> <li>Up sampling operation</li> </ol> </li> <li> <p>Finally, a ResNet/ConvNeXT block followed by a conv layer is applied</p> </li> </ul> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># # UNet
# # Should use ResNet for the starter
</span>
<span class="k">class</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">init_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_mults</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
        <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">with_time_emb</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">resnet_block_groups</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">use_convnext</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">convnext_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># determine dimensions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>

        <span class="n">init_dim</span> <span class="o">=</span> <span class="nf">default</span><span class="p">(</span><span class="n">init_dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">init_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">init_dim</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_dim</span><span class="p">,</span> <span class="o">*</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">dim_mults</span><span class="p">)]</span>
        <span class="n">in_out</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
        <span class="c1"># print(in_out)
</span>        
        <span class="k">if</span> <span class="n">use_convnext</span><span class="p">:</span>
            <span class="n">block_klass</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">ConvNextBlock</span><span class="p">,</span> <span class="n">mult</span><span class="o">=</span><span class="n">convnext_mult</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">block_klass</span> <span class="o">=</span> <span class="nf">partial</span><span class="p">(</span><span class="n">ResNetBlock</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">resnet_block_groups</span><span class="p">)</span>

        <span class="c1"># time embeddings
</span>        <span class="k">if</span> <span class="n">with_time_emb</span><span class="p">:</span>
            <span class="n">time_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">4</span>
            <span class="n">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
                <span class="nc">TimeStepEmbedding</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">time_dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">time_dim</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="n">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># layers
</span>        <span class="n">self</span><span class="p">.</span><span class="n">downs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">ups</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([])</span>
        <span class="n">num_resolutions</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">in_out</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">in_out</span><span class="p">):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">downs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nf">block_klass</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nf">block_klass</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nc">Residual</span><span class="p">(</span><span class="nc">GNorm</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="nc">LinearAttention</span><span class="p">(</span><span class="n">dim_out</span><span class="p">))),</span>
                        <span class="nc">Downsample</span><span class="p">(</span><span class="n">dim_out</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">mid_dim</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_block1</span> <span class="o">=</span> <span class="nf">block_klass</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_attn</span> <span class="o">=</span> <span class="nc">Residual</span><span class="p">(</span><span class="nc">GNorm</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">)))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mid_block2</span> <span class="o">=</span> <span class="nf">block_klass</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="n">in_out</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">self</span><span class="p">.</span><span class="n">ups</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="nf">block_klass</span><span class="p">(</span><span class="n">dim_out</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nf">block_klass</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="nc">Residual</span><span class="p">(</span><span class="nc">GNorm</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="nc">LinearAttention</span><span class="p">(</span><span class="n">dim_in</span><span class="p">))),</span>
                        <span class="nc">Upsample</span><span class="p">(</span><span class="n">dim_in</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Identity</span><span class="p">(),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">out_dim</span> <span class="o">=</span> <span class="nf">default</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">final_conv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nf">block_klass</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">init_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_mlp</span><span class="p">(</span><span class="n">time</span><span class="p">)</span> <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">time_mlp</span><span class="p">)</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="n">h</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># downsample
</span>        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">downsample</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">downs</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">h</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># bottleneck
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mid_block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="c1"># print('mid x: ', x.shape)
</span>
        <span class="c1"># upsample
</span>        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">upsample</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">ups</span><span class="p">:</span>
            <span class="c1"># print('x: ', x.shape, 'h: ', h[-1].shape)
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="nf">pop</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">upsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Testing the UNet
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Batch images: </span><span class="sh">'</span><span class="p">,</span> <span class="n">batch_image</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">unet</span> <span class="o">=</span> <span class="nc">UNet</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">use_convnext</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Unet with ConvNeXT blocks
# cn_unet = UNet(32, use_convnext=True)
</span><span class="n">unet_out</span> <span class="o">=</span> <span class="nf">unet</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">batch_t</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">unet_out</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Parameters = </span><span class="sh">'</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">unet</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">))</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Batch</span> <span class="n">images</span><span class="p">:</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">44</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">Parameters</span> <span class="o">=</span>  <span class="mi">14733783</span></code></pre></figure> <hr> <h3 id="2-forward-diffusion-fixed">2. Forward Diffusion (Fixed)</h3> <p>Gradually add noises to the imput data (images) in a number (\(T\)) of steps. Note that this forward process is designed and scheduled but the result of this forward process is stochastic due to the noises.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># linear beta
</span><span class="k">def</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span> 
    <span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">0.02</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>

<span class="n">T</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># define betas
</span><span class="n">betas</span> <span class="o">=</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>

<span class="c1"># define alphas
</span><span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">betas</span>
<span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alphas_cumprod_prev</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="c1"># shifting alphas_cumprod right
</span><span class="n">sqrt_recip_alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">alphas</span><span class="p">)</span>

<span class="c1"># calculate for diffusion q(x_t | x_{t-1}) and others
</span><span class="n">sqrt_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">)</span>
<span class="n">sqrt_one_minus_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)</span>

<span class="c1"># calculate for posterior q(x_{t-1} | x_t, t_0)
</span><span class="n">posterior_variance</span> <span class="o">=</span> <span class="n">betas</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod_prev</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)</span>

<span class="c1"># getting the preset values in the sampling stage
</span><span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="nf">cpu</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))).</span><span class="nf">to</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>


<span class="c1"># Illustrate with Image
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">http://images.cocodataset.org/val2017/000000039769.jpg</span><span class="sh">'</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nf">open</span><span class="p">(</span><span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">raw</span><span class="p">)</span>
<span class="n">image</span></code></pre></figure> <p>Output:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/diffusion/cats-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/diffusion/cats-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/diffusion/cats-1400.webp"></source> <img src="/assets/img/posts/diffusion/cats.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>To standardize the images, we use the following to make images the same size: <code class="language-plaintext highlighter-rouge">128 x 128</code></p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># image data processing
</span>
<span class="kn">from</span> <span class="n">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">ToPILImage</span><span class="p">,</span> <span class="n">CenterCrop</span><span class="p">,</span> <span class="n">Resize</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Original Image size = </span><span class="sh">'</span><span class="p">,</span> <span class="nc">ToTensor</span><span class="p">()(</span><span class="n">image</span><span class="p">).</span><span class="n">shape</span><span class="p">)</span>

<span class="n">image_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># a series of transformations
</span><span class="n">transform</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span><span class="nc">Resize</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span> 
                     <span class="nc">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
                     <span class="nc">ToTensor</span><span class="p">(),</span> <span class="c1"># turn into torch tensor of shape (Height x Width x Channel), divide by 255 in RGB
</span>                     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)])</span>

<span class="n">reverse_transform</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span> 
                             <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># C x H x W to H x W x C
</span>                             <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span> <span class="o">*</span> <span class="mf">255.0</span><span class="p">),</span>
                             <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)),</span>
                             <span class="nc">ToPILImage</span><span class="p">()])</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="nf">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">New Image Size = </span><span class="sh">'</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">x_start</span> <span class="o">=</span> <span class="n">x_start</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># adding the batch axis
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Batched New Image Size = </span><span class="sh">'</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Original</span> <span class="n">Image</span> <span class="n">size</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">480</span><span class="p">,</span> <span class="mi">640</span><span class="p">])</span>
<span class="n">New</span> <span class="n">Image</span> <span class="n">Size</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">Batched</span> <span class="n">New</span> <span class="n">Image</span> <span class="n">Size</span> <span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span></code></pre></figure> <p>The forward diffusion process of the image is gradually adding noise to the signal until the signal is destroyed.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># sample the time step t given x0
</span><span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span> 
    
    <span class="k">if</span> <span class="ow">not</span> <span class="nf">exists</span><span class="p">(</span><span class="n">noise</span><span class="p">):</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    <span class="n">sqrt_alphas_cumprod_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">sqrt_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">sqrt_alphas_cumprod_t</span> <span class="o">*</span> <span class="n">x0</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">*</span> <span class="n">noise</span>
    

<span class="k">def</span> <span class="nf">get_noisy_image</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span> 
    <span class="c1"># add noise
</span>    <span class="n">x_noisy</span> <span class="o">=</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">reverse_transform</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">())</span>


<span class="c1"># plotting the images from a list
</span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">with_orig</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">row_title</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">imshow_kwargs</span><span class="p">):</span> 
    
    <span class="c1"># make 2d even if there is just 1 row
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span> <span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">imgs</span><span class="p">]</span>
    
    <span class="n">num_rows</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
    <span class="n">num_cols</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">with_orig</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">200</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">num_cols</span><span class="p">,</span> <span class="n">squeeze</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span> 
        <span class="n">row</span> <span class="o">=</span> <span class="p">[</span><span class="n">image</span><span class="p">]</span> <span class="o">+</span> <span class="n">row</span> <span class="k">if</span> <span class="n">with_orig</span> <span class="k">else</span> <span class="n">row</span>
        <span class="k">for</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">row</span><span class="p">):</span> 
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="o">**</span><span class="n">imshow_kwargs</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="n">xticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    
    <span class="k">if</span> <span class="n">with_orig</span><span class="p">:</span> 
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">'</span><span class="s">Original Image</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="n">title</span><span class="p">.</span><span class="nf">set_size</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="nf">exists</span><span class="p">(</span><span class="n">row_title</span><span class="p">):</span> 
        <span class="k">for</span> <span class="n">row_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_rows</span><span class="p">):</span> 
            <span class="n">axes</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">row_title</span><span class="p">[</span><span class="n">row_idx</span><span class="p">])</span>
    
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    
<span class="c1"># plot the forward diffusion process
</span><span class="n">imgs</span> <span class="o">=</span> <span class="p">[[</span><span class="nf">get_noisy_image</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">i</span> <span class="o">+</span> <span class="mi">45</span> <span class="o">*</span> <span class="n">j</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span>
<span class="n">row_title</span> <span class="o">=</span> <span class="p">[</span><span class="mi">45</span> <span class="o">*</span> <span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">45</span><span class="p">)]</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">row_title</span><span class="o">=</span><span class="n">row_title</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span></code></pre></figure> <p>Output:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/diffusion/cats2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/diffusion/cats2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/diffusion/cats2-1400.webp"></source> <img src="/assets/img/posts/diffusion/cats2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <hr> <h3 id="3-the-loss-function">3. The Loss Function</h3> <p>The loss function is the <code class="language-plaintext highlighter-rouge">L2</code> or <code class="language-plaintext highlighter-rouge">L1</code> loss between the <code class="language-plaintext highlighter-rouge">added noise</code> and the <code class="language-plaintext highlighter-rouge">predicted noise</code>. People have found that <code class="language-plaintext highlighter-rouge">Huber loss</code> gives better results because it’s less sensitive to outliers in the pixels that might be introduced once in a while by Gaussian.</p> <p>Note that we train the noise predictor $\epsilon_\theta(x_t, t)$ to achieve this noise-2-noise prediction task</p> <p>And here, $\epsilon_\theta$ is the UNet.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># This is the loss function 
# Potentially, we have to add more terms, such as KLDiv to the loss function
</span><span class="k">def</span> <span class="nf">p_losses</span><span class="p">(</span><span class="n">denoising_model</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">L1</span><span class="sh">'</span><span class="p">):</span> 
    
    <span class="k">assert</span> <span class="n">loss_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">L1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">L2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">huber</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nf">exists</span><span class="p">(</span><span class="n">noise</span><span class="p">):</span> <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
    
    <span class="n">x_noisy</span> <span class="o">=</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="nf">denoising_model</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    
    <span class="c1"># print(x_noisy.shape)
</span>    <span class="c1"># print(predicted_noise.shape)
</span>    
    <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">L1</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">l1_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">L2</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">huber</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> <span class="k">raise</span> <span class="nc">NotADirectoryError</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">loss</span>


<span class="c1"># test loss
</span><span class="n">unet</span> <span class="o">=</span> <span class="nc">UNet</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">use_convnext</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># batch_size = 64, channel = 3
</span><span class="n">x0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="c1"># for each batch, tag time to it
</span><span class="n">ts</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">))</span>
<span class="nf">p_losses</span><span class="p">(</span><span class="n">unet</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">L1</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.9898</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">L1LossBackward0</span><span class="o">&gt;</span><span class="p">)</span></code></pre></figure> <hr> <h3 id="before-going-too-far">Before Going Too Far..</h3> <p>So now we have a forward diffusion process, a noise-2-noise model and a loss to train the model.</p> <p>If we have <code class="language-plaintext highlighter-rouge">10</code> images, <code class="language-plaintext highlighter-rouge">T=200</code> diffusion steps, one epoch needs to go through <code class="language-plaintext highlighter-rouge">2000</code> images with different noise levels, this number scales with the number of images ….</p> <p>In training, typically this is a batch <code class="language-plaintext highlighter-rouge">b</code>:</p> <ul> <li><code class="language-plaintext highlighter-rouge">t0, img3 | t0</code></li> <li><code class="language-plaintext highlighter-rouge">t1, img1 | t1</code></li> <li><code class="language-plaintext highlighter-rouge">t2, img9 | t2</code></li> <li><code class="language-plaintext highlighter-rouge">t3, img1 | t3</code></li> </ul> <p>…</p> <ul> <li><code class="language-plaintext highlighter-rouge">tb, img8 | tb</code></li> </ul> <p>So different from supervised label prediction, the model needs to be trained using a large number of epochs.</p> <hr> <h3 id="4-data-and-datalaoder-for-diffusion">4. Data and Datalaoder for diffusion</h3> <p>I used the flower dataset for playing with the DDPM model.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">dataset</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">huggan/flowers-102-categories</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">]))</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="n">i</span><span class="p">.</span><span class="nf">item</span><span class="p">()][</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8189</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="p">))]</span>
<span class="c1"># imgs = [i.item() for i in torch.randint(0, 8189, (10, ))]
</span><span class="nf">plot</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="mi">8189</span></code></pre></figure> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/diffusion/flowers-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/diffusion/flowers-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/diffusion/flowers-1400.webp"></source> <img src="/assets/img/posts/diffusion/flowers.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Still, they are of different sizes and need to be standardized:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">IMAGE_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">CHANNELS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">flower_transformation</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
    <span class="nc">Resize</span><span class="p">(</span><span class="n">IMAGE_SIZE</span><span class="p">),</span>
    <span class="nc">CenterCrop</span><span class="p">(</span><span class="n">IMAGE_SIZE</span><span class="p">),</span>
    <span class="nc">ToTensor</span><span class="p">(),</span> <span class="c1"># turn into Numpy array of shape HWC, divide by 255
</span>    <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    
<span class="p">])</span>

<span class="n">flower_reverse_transformation</span> <span class="o">=</span> <span class="nc">Compose</span><span class="p">([</span>
     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span>
     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="nf">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># CHW to HWC
</span>     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span> <span class="o">*</span> <span class="mf">255.</span><span class="p">),</span>
     <span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)),</span>
     <span class="nc">ToPILImage</span><span class="p">(),</span>
<span class="p">])</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">][</span><span class="mi">999</span><span class="p">][</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">]</span>
<span class="n">x_start</span> <span class="o">=</span> <span class="nf">flower_transformation</span><span class="p">(</span><span class="n">x_start</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nf">flower_reverse_transformation</span><span class="p">(</span><span class="n">x_start</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="nf">flower_transformation</span><span class="p">(</span><span class="n">example</span><span class="p">[</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">])</span> <span class="k">for</span> <span class="n">example</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">])</span>


<span class="n">dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">],</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">dataloader length = </span><span class="sh">'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">()</span>
<span class="nf">flower_reverse_transformation</span><span class="p">(</span><span class="n">x_start</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:])</span>

<span class="c1"># transformed_dataset = dataset.with_transform(flower_transformation)</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="n">dataloader</span> <span class="n">length</span> <span class="o">=</span>  <span class="mi">128</span>
<span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span></code></pre></figure> <hr> <h3 id="5-sampling-generating">5. Sampling (Generating)</h3> <p>The sampling is to get a batch of images and time batch. The sampling can be done with <code class="language-plaintext highlighter-rouge">no_grad()</code> for speeding up.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">t_index</span><span class="p">):</span> 
    
    <span class="n">betas_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sqrt_recip_alphas_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">sqrt_recip_alphas</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="c1"># Equation 11 in Ho et al. 2020
</span>    <span class="c1"># Use the noise-2-noise model to predict the mean
</span>    <span class="n">model_mean</span> <span class="o">=</span> <span class="n">sqrt_recip_alphas_t</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">betas_t</span> <span class="o">*</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt_one_minus_alphas_cumprod_t</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">t_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="k">return</span> <span class="n">model_mean</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">posterior_variance_t</span> <span class="o">=</span> <span class="nf">extract</span><span class="p">(</span><span class="n">posterior_variance</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model_mean</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">posterior_variance_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
    

<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span> 
    
    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>
    
    <span class="n">b</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># start with pure noise (for each example in the batch)
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">imgs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">)),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">'</span><span class="s">Sampling Loop Timestep</span><span class="sh">'</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">T</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="p">),</span> <span class="n">i</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">imgs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
    
    <span class="k">return</span> <span class="n">imgs</span>

<span class="nd">@torch.no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">CHANNELS</span><span class="p">):</span> 
    <span class="k">return</span> <span class="nf">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">))</span></code></pre></figure> <p>The function <code class="language-plaintext highlighter-rouge">sample</code> is sampling pure Gaussian noises at \(t=T\) and then denoising it back to the original signal in a generative way.</p> <hr> <h3 id="6-training">6. Training</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Train Model using DataParallel
# Better way is to use DDP
</span><span class="n">device</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span>

<span class="n">noise2noise</span> <span class="o">=</span> <span class="nc">UNet</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">CHANNELS</span><span class="p">,</span> <span class="n">dim_mults</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">use_convnext</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of Parameters = </span><span class="sh">'</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">noise2noise</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">))</span>

<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Using</span><span class="sh">"</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">device_count</span><span class="p">(),</span> <span class="sh">"</span><span class="s">GPUs!</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">noise2noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">DataParallel</span><span class="p">(</span><span class="n">noise2noise</span><span class="p">)</span>

<span class="n">noise2noise</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">noise2noise</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="c1"># train on!!
</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">],</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">192</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">dataloader length = </span><span class="sh">'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>

<span class="c1"># epochs need to be T * normal_epochs
# so that almost all the noisy images will get sampled
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
    
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch_images</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_images</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">batch_images</span> <span class="o">=</span> <span class="n">batch_images</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Algorithm 1 line 3: sample t uniformally for every example in the batch
</span>        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="nf">p_losses</span><span class="p">(</span><span class="n">noise2noise</span><span class="p">,</span> <span class="n">batch_images</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">L1</span><span class="sh">'</span><span class="p">)</span>

        <span class="c1"># if step % 100 == 0: print(f'Epoch: {epoch:02d} | Step: {step:03d} | Loss: {loss.item():.3f}')
</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">:</span><span class="mi">02</span><span class="n">d</span><span class="si">}</span><span class="s"> | Step: </span><span class="si">{</span><span class="n">step</span><span class="si">:</span><span class="mi">03</span><span class="n">d</span><span class="si">}</span><span class="s"> | Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="nf">now</span><span class="p">().</span><span class="nf">strftime</span><span class="p">(</span><span class="sh">"</span><span class="s">%Y-%m-%d-%H-%M-%S</span><span class="sh">"</span><span class="p">)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">Diffusion_Flower_</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">now</span><span class="si">}</span><span class="sh">'</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">noise2noise</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">'</span><span class="s">/home/ubuntu/trained_models/</span><span class="sh">'</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">.pt</span><span class="sh">'</span><span class="p">))</span></code></pre></figure> <p>Output:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Number</span> <span class="n">of</span> <span class="n">Parameters</span> <span class="o">=</span>  <span class="mi">32009619</span>
<span class="n">Using</span> <span class="mi">8</span> <span class="n">GPUs</span><span class="err">!</span>
<span class="n">dataloader</span> <span class="n">length</span> <span class="o">=</span>  <span class="mi">43</span>

<span class="n">Epoch</span><span class="p">:</span> <span class="mi">00</span> <span class="o">|</span> <span class="n">Step</span><span class="p">:</span> <span class="mi">042</span> <span class="o">|</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.801</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">05</span> <span class="o">|</span> <span class="n">Step</span><span class="p">:</span> <span class="mi">042</span> <span class="o">|</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.022</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">10</span> <span class="o">|</span> <span class="n">Step</span><span class="p">:</span> <span class="mi">042</span> <span class="o">|</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.416</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">15</span> <span class="o">|</span> <span class="n">Step</span><span class="p">:</span> <span class="mi">042</span> <span class="o">|</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.206</span>
<span class="bp">...</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">835</span> <span class="o">|</span> <span class="n">Step</span><span class="p">:</span> <span class="mi">042</span> <span class="o">|</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.065</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">840</span> <span class="o">|</span> <span class="n">Step</span><span class="p">:</span> <span class="mi">042</span> <span class="o">|</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.067</span></code></pre></figure> <p>The DDPM model might also be quite memory expensive due to the <code class="language-plaintext highlighter-rouge">IMAGE_SIZE</code> and the <code class="language-plaintext highlighter-rouge">Attention</code> mechanism in the UNet (on top of the <code class="language-plaintext highlighter-rouge">ResNet</code> parameters..).</p> <p>batch size &gt; 24: memory explodes for 1 GPU</p> <ul> <li>current speed = 240s / epoch @ batch_size = 24 on 1 GPU</li> <li>current speed = 110s / epoch @ batch_size = 64 on 4 GPUs</li> <li>current speed = 110s / epoch @ batch_size = 96 on 4 GPUs</li> <li>current speed = 116s / epoch @ batch_size = 192 on 8 GPUs</li> </ul> <hr> <h3 id="7-generating">7. Generating</h3> <p>After the model got trained, it’ll be interesting to take a look at some of the generated images:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># sample 64 images
</span><span class="n">samples</span> <span class="o">=</span> <span class="nf">sample</span><span class="p">(</span><span class="n">noise2noise</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">CHANNELS</span><span class="p">)</span>
<span class="n">ts</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">T</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.75</span> <span class="o">*</span> <span class="n">T</span><span class="p">),</span> <span class="nf">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="n">T</span><span class="p">),</span> <span class="n">T</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">samples_for_plot</span> <span class="o">=</span> <span class="p">[[</span><span class="nf">flower_reverse_transformation</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">6</span> <span class="o">*</span> <span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]))</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">ts</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">samples_for_plot</span><span class="p">)</span></code></pre></figure> <p>Output:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/diffusion/sample1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/diffusion/sample1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/diffusion/sample1-1400.webp"></source> <img src="/assets/img/posts/diffusion/sample1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/diffusion/sample2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/diffusion/sample2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/diffusion/sample2-1400.webp"></source> <img src="/assets/img/posts/diffusion/sample2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/diffusion/sample3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/diffusion/sample3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/diffusion/sample3-1400.webp"></source> <img src="/assets/img/posts/diffusion/sample3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <h3 id="8-notes">8. Notes</h3> <p>The generative power of DDPM is really strong and the model formulation is cool and with physics intuition behind it.</p> <p>The <code class="language-plaintext highlighter-rouge">Time Embedding</code> dictates the <code class="language-plaintext highlighter-rouge">UNet</code> to be lowpass-like filter in the later time points (\(t \sim T\)), generating contours or global shapes from the noises. In the earlier time points (\(t &lt;&lt; T\)), it performs like a highpass filter, denoising the high frequency noises away and resulting in a high resolution images that approximates the image distribution of the training data set. One can attenuate the denoising power, say, making it less stochastic by averaging 10 or more denoised images together at one step. It will make the image less <code class="language-plaintext highlighter-rouge">hallucinated</code> but also more blurry as one tries to averaging out some high-resolution signals that correspond to different end results. It is also the tradeoff between FID and IS in generative models</p> <hr> <h3 id="references">References</h3> <ol> <li>Jonathan Ho, Ajay Jain, Pieter Abbeel, Denoising Diffusion Probabilistic Models, https://arxiv.org/abs/2006.11239, 2020</li> <li>Calvin Luo, Understanding Diffusion Models: A Unified Perspective, https://arxiv.org/abs/2208.11970, 2022</li> <li>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, Attention Is All You Need, https://arxiv.org/abs/1706.03762, 2017</li> <li>https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</li> <li>https://huggingface.co/blog/annotated-diffusion</li> </ol> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yen-Lin Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>