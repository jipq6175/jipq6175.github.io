<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Discrete Flow Matching | Yen-Lin Chen</title> <meta name="author" content="Yen-Lin Chen"> <meta name="description" content="Playing with discrete flow matching on toy examples"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jipq6175.github.io/blog/2025/discrete_flow_matching/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Yen-Lin </span>Chen</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Discrete Flow Matching</h1> <p class="post-meta">June 27, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/reading"> <i class="fas fa-hashtag fa-sm"></i> reading</a>   <a href="/blog/tag/generating"> <i class="fas fa-hashtag fa-sm"></i> generating</a>   <a href="/blog/tag/coding"> <i class="fas fa-hashtag fa-sm"></i> coding</a>     ·   <a href="/blog/category/models"> <i class="fas fa-tag fa-sm"></i> models</a>   </p> </header> <article class="post-content"> <p>We will explore the generative discrete flow matching model on <code class="language-plaintext highlighter-rouge">2D Checkerboard</code> and <code class="language-plaintext highlighter-rouge">Sequence</code> data.</p> <p>One benefit of DFM is to compute the ELBO or log-probability for any given data, by forward solving ODE using the trained model.</p> <p>This notebook is the standalone version for future references.</p> <h1 id="table-of-contents">Table of Contents:</h1> <ol> <li> <p>Discrete Frameworks</p> <p>1.1 Scheduler</p> <p>1.2 Mixture of discrete paths</p> <p>1.3 Training losses</p> <p>1.4 ODE/SDE solvers for DFM</p> </li> <li>Training Pipeline</li> <li>ELBO estimates</li> </ol> <p>The above will be applied to 2 cases: <code class="language-plaintext highlighter-rouge">2D</code> and <code class="language-plaintext highlighter-rouge">Sequence</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>

<span class="kn">import</span> <span class="n">os</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">torch</span><span class="p">,</span> <span class="n">einops</span><span class="p">,</span> <span class="n">copy</span>

<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">matplotlib.cm</span> <span class="k">as</span> <span class="n">cm</span>

<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>

<span class="kn">from</span> <span class="n">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="n">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="n">contextlib</span> <span class="kn">import</span> <span class="n">nullcontext</span>
<span class="kn">from</span> <span class="n">math</span> <span class="kn">import</span> <span class="n">ceil</span>
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">)</span>

<span class="n">SAVE_PATH</span> <span class="o">=</span> <span class="sh">'</span><span class="s">discrete_flow/</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">SAVE_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h2 id="1-discrete-frameworks">1. Discrete Frameworks</h2> <p>In the continuous case, at train time we do the following:</p> <ol> <li>Sample \(t\in[0,1]\)</li> <li>Sample data point \(x_1\sim q(x)\)</li> <li> <table> <tbody> <tr> <td>Sample $$x \sim p_t(x</td> <td>x_1)\(given some\)p_t$$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Compute corresponding vector field $$u_t(x</td> <td>x_1)$$</td> </tr> </tbody> </table> </li> <li>Use neural network \(v_{t,\theta}(x)\) to regress on the vector field</li> </ol> <p>In the discrete case, steps 1-2 remain and we need to massage the following steps:</p> <ol> <li>Sample \(x \sim p_t(x\|x_1)\) given some \(p_t\) using mixture of discrete path.</li> </ol> <p>4-5. Instead of regress on the flow vector field, we predict the \(x_1\) given \(x_t\) with typical cross-entropy loss or generalized KL-loss.</p> <p>We first take a look at the schedulers, which are identical to the continuous case. The scheduler holds the time-dependent mean \(\alpha_t\) and variance \(\sigma_t\) that models the normal distribution</p> \[p_t(x|x_1)\sim\mathcal{N}(x| \alpha_t x_1, \sigma_t^2I)\] <p>Again, \(\alpha_t\) and \(\sigma_t\) need to satisfy the boundary condition: \(\alpha_1 = \sigma_0 = 1\) and \(\alpha_0 = \sigma_1 = 0\)</p> <p>In the continuous case, the source distribution, \(p_0\) is usually signal-less by design to be a normal Gaussian distribution. In the discrete case, the source distribution at each position can be 2 cases:</p> <ol> <li>Uniform distribution over the discrete space or the vocabularies.</li> <li>Mask token</li> </ol> <p>Both source distributions provide signal-less and can be coupled with data distribution via any schedulers.</p> <h3 id="11-scheduler">1.1 Scheduler</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">SchedulerOutput</span><span class="p">:</span> 
    
    <span class="n">alpha_t</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">sigma_t</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">d_alpha_t</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">d_sigma_t</span><span class="p">:</span> <span class="n">Tensor</span>
        

<span class="k">class</span> <span class="nc">Scheduler</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">Scheduler Base
       p_t(x | x_1) = N(x | alpha_t * x_1, sigma_t^2 * I)
    </span><span class="sh">'''</span>
    
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SchedulerOutput</span><span class="p">:</span>
        <span class="k">pass</span>


<span class="k">class</span> <span class="nc">ConditionalOTScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">Conditional OT Scheduler
       p_t(x | x_1) = N(x | t x_1, (1-t)^2 I)
    </span><span class="sh">'''</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SchedulerOutput</span><span class="p">:</span> 
        <span class="k">return</span> <span class="nc">SchedulerOutput</span><span class="p">(</span><span class="n">alpha_t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> 
                               <span class="n">sigma_t</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span><span class="p">,</span> 
                               <span class="n">d_alpha_t</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">t</span><span class="p">),</span>
                               <span class="n">d_sigma_t</span><span class="o">=-</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>

    
<span class="k">class</span> <span class="nc">PolynomialScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">Polynomial Scheduler
       p_t(x | x_1) = N(x | t^n x_1, (1-t^n)^2 I)
    </span><span class="sh">'''</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span> 
        <span class="k">assert</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="p">(</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">))</span>
        <span class="k">assert</span> <span class="n">n</span> <span class="o">&gt;</span> <span class="mf">0.0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">n</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SchedulerOutput</span><span class="p">:</span> 
        <span class="n">n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n</span>
        <span class="k">return</span> <span class="nc">SchedulerOutput</span><span class="p">(</span><span class="n">alpha_t</span><span class="o">=</span><span class="n">t</span> <span class="o">**</span> <span class="n">n</span><span class="p">,</span> 
                               <span class="n">sigma_t</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">t</span> <span class="o">**</span> <span class="n">n</span><span class="p">,</span> 
                               <span class="n">d_alpha_t</span><span class="o">=</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)),</span>
                               <span class="n">d_sigma_t</span><span class="o">=-</span><span class="n">n</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">**</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)))</span>
    
    
<span class="k">class</span> <span class="nc">CosineScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">Cosine Scheduler
       p_t(x | x_1) = N(x | sin(pi*t/2) x_1, cos(pi*t/2)^2 I)
    </span><span class="sh">'''</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SchedulerOutput</span><span class="p">:</span> 
        <span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">pi</span>
        <span class="k">return</span> <span class="nc">SchedulerOutput</span><span class="p">(</span><span class="n">alpha_t</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">t</span><span class="p">),</span>
                               <span class="n">sigma_t</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">t</span><span class="p">),</span> 
                               <span class="n">d_alpha_t</span><span class="o">=</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">t</span><span class="p">),</span>
                               <span class="n">d_sigma_t</span><span class="o">=-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">t</span><span class="p">))</span>
</code></pre></div></div> <h3 id="12-mixture-of-discrete-path">1.2 Mixture of Discrete Path</h3> <p>We denote a sequence \(x\) as an array with \(N\) element: \(x = (x_1, x_2, x_3, ..., x_N)\) where each element takes on discrete value from a set of vocabulary of size \(d\). The sequence space is then \(d^N\).</p> <p>Given samples from source and target distributions, \(x_0\) and \(x_1\), and any data coupling \(\pi(x_0, x_1)\), the probability path \(p_t\) can be represented with marginal probability paths:</p> \[p_t(x) = \sum_{x_0, x_1}p_t(x \mid x_0, x_1)\pi(x_0, x_1)\] <p>Since \(x\) is an N-dim array, we can further represent the marginal probability paths using the mixture of its individual components:</p> \[p_t(x|x_0, x_1) = \prod_{i=1}^N p_t(x^i \mid x_0, x_1)\] <p>\(p_t(x^i \mid x_0, x_1)\) is a time-dependent probability on the vocabulary set with boundary conditions defined by the source and target:</p> \[p_0(x^i \mid x_0, x_1) = \delta_{x_0}(x^i)\] \[p_1(x^i \mid x_0, x_1) = \delta_{x_1}(x^i)\] <p>where \(\delta_y(x^i) = 1\) if \(x^i = y^i\) and \(0\) otherwise.</p> <p>Then we can use a convex linear combination (similar to those in continuous case) to represent the individual one of a mixture of discrete paths:</p> \[p_t(x^i \mid x_0, x_1) = (1-\kappa_t)p_0(x^i \mid x_0, x_1) + \kappa_t p_1(x^i \mid x_0, x_1) = (1-\kappa_t)\delta_{x_0}(x^i) + \kappa_t \delta_{x_1}(x^i)\] <p>with \(0 &lt; \kappa_t &lt; 1\), \(\kappa_0 = 0\), \(\kappa_1 = 1\) and monotonically increasing.</p> <p>This individual marginal probability path for position \(i\) indicates that given time \(t\) and \(x_0\) and \(x_1\), \(x^i\) only got 2 choices: \(x_0^i\) with probability \(\kappa_t\) and \(x_1^i\) with probability \(1-\kappa_t\), i.e. \(x_i\) assumes either the source or target with time-dependent probability.</p> <p>The conditional marginal generating (forward) velocity is then</p> \[u_t^i(x^i \mid z) = \frac{\dot{\kappa_t}}{1 - \kappa_t}\left[p_{1 \mid t}(x^i|z) - \delta_z(x^i) \right]\] <p>(See Gat. et al 2024 for derivation)</p> <p>This velocity is used then the model is trained to be the denoiser (as compared to noise-prediction.) The \(\kappa_t\) is from the scheduler and \(p_{1 \mid t}(x^i \mid x)\) is the posterior probability defined on the vocabulary set of size \(d\). Essentially, this is from the trained neural network given noised sequence \(x_t\) at time \(t\) and predicting the posterior of the clean sequence \(x_1\). \(\delta_z(x^i)\) is the one-hot probability of \(x_t\). So this velocity makes \(x_t\) move toward predicted \(x_1\) at sampling. Note that \(\kappa_t = 1\) when \(t=1\) is a singularity for the generating velocity, so we typically do the sampling till \(t = 1-\epsilon\) and use \(p_{1 \mid t=1-\epsilon}(x^i \mid x)\) as the sample at \(x_1\).</p> <p>The reverse velocity is then</p> \[u_t^i(x^i \mid z) = \frac{\dot{\kappa_t}}{\kappa_t}\left[\delta_z(x^i) - p_{0 \mid t}(x^i \mid x) \right]\] <p>which will be used for corrector sampling during the generating/inferencing process.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">unsqueeze_to_match</span><span class="p">(</span><span class="n">source</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">how</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">suffix</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Unsqueeze the source tensor to match the dimensionality of the target tensor.

    Args:
        source (Tensor): The source tensor to be unsqueezed.
        target (Tensor): The target tensor to match the dimensionality of.
        how (str, optional): Whether to unsqueeze the source tensor at the beginning
            (</span><span class="sh">"</span><span class="s">prefix</span><span class="sh">"</span><span class="s">) or end (</span><span class="sh">"</span><span class="s">suffix</span><span class="sh">"</span><span class="s">). Defaults to </span><span class="sh">"</span><span class="s">suffix</span><span class="sh">"</span><span class="s">.

    Returns:
        Tensor: The unsqueezed source tensor.
    </span><span class="sh">"""</span>
    <span class="nf">assert </span><span class="p">(</span>
        <span class="n">how</span> <span class="o">==</span> <span class="sh">"</span><span class="s">prefix</span><span class="sh">"</span> <span class="ow">or</span> <span class="n">how</span> <span class="o">==</span> <span class="sh">"</span><span class="s">suffix</span><span class="sh">"</span>
    <span class="p">),</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">how</span><span class="si">}</span><span class="s"> is not supported, only </span><span class="sh">'</span><span class="s">prefix</span><span class="sh">'</span><span class="s"> and </span><span class="sh">'</span><span class="s">suffix</span><span class="sh">'</span><span class="s"> are supported.</span><span class="sh">"</span>

    <span class="n">dim_diff</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="n">source</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">dim_diff</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">how</span> <span class="o">==</span> <span class="sh">"</span><span class="s">prefix</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">source</span> <span class="o">=</span> <span class="n">source</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">how</span> <span class="o">==</span> <span class="sh">"</span><span class="s">suffix</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">source</span> <span class="o">=</span> <span class="n">source</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">source</span>



<span class="k">def</span> <span class="nf">expand_tensor_like</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">expand_to</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">`input_tensor` is a 1d vector of length equal to the batch size of `expand_to`,
    expand `input_tensor` to have the same shape as `expand_to` along all remaining dimensions.

    Args:
        input_tensor (Tensor): (batch_size,).
        expand_to (Tensor): (batch_size, ...).

    Returns:
        Tensor: (batch_size, ...).
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">Input tensor must be a 1d vector.</span><span class="sh">"</span>
    <span class="nf">assert </span><span class="p">(</span>
        <span class="n">input_tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">expand_to</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">),</span> <span class="sa">f</span><span class="sh">"</span><span class="s">The first (batch_size) dimension must match. Got shape </span><span class="si">{</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> and </span><span class="si">{</span><span class="n">expand_to</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span>

    <span class="n">dim_diff</span> <span class="o">=</span> <span class="n">expand_to</span><span class="p">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="n">input_tensor</span><span class="p">.</span><span class="n">ndim</span>

    <span class="n">t_expanded</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">t_expanded</span> <span class="o">=</span> <span class="n">t_expanded</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">dim_diff</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">t_expanded</span><span class="p">.</span><span class="nf">expand_as</span><span class="p">(</span><span class="n">expand_to</span><span class="p">)</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">PathSample</span><span class="p">:</span> 
    <span class="sh">'''</span><span class="s">Sample of conditional probability path</span><span class="sh">'''</span>

    <span class="n">x_1</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">x_0</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">dx_t</span><span class="p">:</span> <span class="n">Tensor</span>
        

<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DiscretePathSample</span><span class="p">:</span> 
    <span class="sh">'''</span><span class="s">Sample of conditional discrete probability path</span><span class="sh">'''</span>
    
    <span class="n">x_1</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">x_0</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span>
        
        

<span class="k">class</span> <span class="nc">ProbPath</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">Probability Path Base Class</span><span class="sh">'''</span>
    
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PathSample</span><span class="p">:</span> 
        <span class="k">pass</span>
    
    <span class="k">def</span> <span class="nf">assert_sample_shape</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span> 
        <span class="k">assert</span> <span class="n">t</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="k">assert</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">assert</span> <span class="n">x_0</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">x_1</span><span class="p">.</span><span class="n">shape</span>
        
        
<span class="c1"># mixture discrete path
</span><span class="k">class</span> <span class="nc">MixtureDiscreteProbPath</span><span class="p">(</span><span class="n">ProbPath</span><span class="p">):</span> 
    <span class="sh">'''</span><span class="s">Mixture Discrete Probability Path</span><span class="sh">'''</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">:</span> <span class="n">Scheduler</span><span class="p">):</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler</span>
        
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DiscretePathSample</span><span class="p">:</span> 
        <span class="n">self</span><span class="p">.</span><span class="nf">assert_sample_shape</span><span class="p">(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">sigma_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="p">).</span><span class="n">sigma_t</span>
        <span class="n">sigma_t</span> <span class="o">=</span> <span class="nf">expand_tensor_like</span><span class="p">(</span><span class="n">sigma_t</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
        
        <span class="c1"># sigma_t determines the probability to stay at source
</span>        <span class="c1"># with probability of 1 - sigma_t it flips to target / data
</span>        <span class="n">source_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x_1</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">sigma_t</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">source_indices</span><span class="p">,</span> <span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="nc">DiscretePathSample</span><span class="p">(</span><span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">posterior_to_velocity</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">posterior_logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 

        <span class="c1"># this is p_{1|t}(x|z)
</span>        <span class="n">posterior</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">posterior_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># this is p_t(x|z)
</span>        <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="nf">unsqueeze_to_match</span><span class="p">(</span><span class="n">source</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">x_t</span><span class="p">)</span>
        
        <span class="n">scheduler_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">kappa_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">alpha_t</span>
        <span class="n">d_kappa_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">d_alpha_t</span>
        
        <span class="nf">return </span><span class="p">(</span><span class="n">d_kappa_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">kappa_t</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">posterior</span> <span class="o">-</span> <span class="n">x_t</span><span class="p">)</span>
</code></pre></div></div> <h3 id="13-training-losses">1.3 Training Losses</h3> <p>For training the probability denoiser, i.e. training a model that reproduces \(p_{1 \mid t}(x^i \mid z)\), the loss takes the form:</p> \[\mathcal{L}(\theta) = -\sum_i \textbf{E}_{t, (X_0, X_1), X_t}\left[\log{p_{1 \mid t}(X_1^i \mid X_t)} \right]\] <p>This is essentially the cross entropy loss. The model is trained to predict the signal sequence \(X_1\) given some noised sequence of \(X_t\). In analogy to image, this is predicting the noise-less image \(X_1\) given some noised images \(X_t\) instead of predicting the flow vector field \(u_t(X_t)\)</p> <p>Alternatively, we can use generalized KL loss, which takes the form:</p> \[\mathcal{L}(\theta) = -\sum_i \textbf{E}_{t, (X_0, X_1), X_t}\frac{\dot{\kappa_t}}{1-\kappa_t}\left[(\delta_{x_1}(x_t^i) - 1)\log{p_{1 \mid t}(x_1^i \mid x_t)} + \delta_{x_1}(x_t^i) - p_{1 \mid t}(x_1^i \mid x_t) \right]\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">modules</span><span class="p">.</span><span class="n">loss</span><span class="p">.</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">A generalized KL loss for discrete flow matching.
    A class that measures the generalized KL of a discrete flow model :math:`p_{1|t}` w.r.t. a probability path given by ``path``. Note: this class is assuming that the model is trained on the same path.

    For a model trained on a space :math:`\mathcal{S} = \mathcal{T}^d`, :math:`\mathcal{T} = [K] = \set{1,2,\ldots,K}`, the loss is given by

    .. math::
            \ell_i(x_1, x_t, t) = -\frac{\dot{\kappa}_t}{1-\kappa_t} \biggr[  p_{1|t}(x_t^i|x_t) -\delta_{x^i_1}(x_t^i) + (1-\delta_{x^i_1}(x_t^i))\left(\log p_{1|t}(x_1^i|x_t)\right)\biggr],

    where :math:`\kappa_t` is the scheduler associated with ``path``.

    Args:
        path (MixtureDiscreteProbPath): Probability path (x-prediction training).
        reduction (str, optional): Specify the reduction to apply to the output ``</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="s">`` | ``</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s">`` | ``</span><span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="s">``. ``</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="s">``: no reduction is applied to the output, ``</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s">``: the output is reduced by mean over sequence elements, ``</span><span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="s">``: the output is reduced by sum over sequence elements. Defaults to </span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s">.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">MixtureDiscreteProbPath</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sa">r</span><span class="sh">"""</span><span class="s">Evaluates the generalized KL loss.

        Args:
            logits (Tensor): posterior model output (i.e., softmax(``logits``) :math:`=p_{1|t}(x|x_t)`), shape (batch, d, K).
            x_1 (Tensor): target data point :math:`x_1 \sim q`, shape (batch, d).
            x_t (Tensor): conditional sample at :math:`x_t \sim p_t(\cdot|x_1)`, shape (batch, d).
            t (Tensor): times in :math:`[0,1]`, shape (batch).

        Raises:
            ValueError: reduction value must be one of ``</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="s">`` | ``</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="s">`` | ``</span><span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="s">``.

        Returns:
            Tensor: Generalized KL loss.
        </span><span class="sh">"""</span>
        <span class="n">x_1_shape</span> <span class="o">=</span> <span class="n">x_1</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1"># extract x_1 value of log(p_{1|t}(x|x_t)).
</span>        <span class="n">log_p_1t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">log_p_1t_x1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">log_p_1t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">x_1</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">log_p_1t_x1</span> <span class="o">=</span> <span class="n">log_p_1t_x1</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">*</span><span class="n">x_1_shape</span><span class="p">)</span>

        <span class="c1"># extract x_t value of p_{1|t}(x|x_t).
</span>        <span class="n">p_1t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">log_p_1t</span><span class="p">)</span>
        <span class="n">p_1t_xt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">p_1t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">x_t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">p_1t_xt</span> <span class="o">=</span> <span class="n">p_1t_xt</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">*</span><span class="n">x_1_shape</span><span class="p">)</span>

        <span class="n">scheduler_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

        <span class="n">jump_coefficient</span> <span class="o">=</span> <span class="p">(</span><span class="n">scheduler_output</span><span class="p">.</span><span class="n">d_alpha_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">alpha_t</span><span class="p">))[(...,)</span> <span class="o">+</span> <span class="p">(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_1</span><span class="p">.</span><span class="nf">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="n">jump_coefficient</span> <span class="o">=</span> <span class="n">jump_coefficient</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">x_1_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">delta_x1_xt</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_t</span> <span class="o">==</span> <span class="n">x_1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">log_p_1t</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">jump_coefficient</span> <span class="o">*</span> <span class="p">(</span><span class="n">p_1t_xt</span> <span class="o">-</span> <span class="n">delta_x1_xt</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">delta_x1_xt</span><span class="p">)</span> <span class="o">*</span> <span class="n">log_p_1t_x1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="sh">"</span><span class="s">none</span><span class="sh">"</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">self</span><span class="p">.</span><span class="n">reduction</span><span class="si">}</span><span class="s"> is not a valid value for reduction</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="14-odesde-solvers">1.4 ODE/SDE Solvers</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sh">"""</span><span class="s">Categorical sampler according to weights in the last dimension of ``probs`` using :func:`torch.multinomial`.

    Args:
        probs (Tensor): probabilities.

    Returns:
        Tensor: Samples.
    </span><span class="sh">"""</span>

    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replacement</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="o">*</span><span class="n">probs</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">get_nearest_times</span><span class="p">(</span><span class="n">time_grid</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t_discretization</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cdist</span><span class="p">(</span>
        <span class="n">time_grid</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">t_discretization</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">compute_mode</span><span class="o">=</span><span class="sh">"</span><span class="s">donot_use_mm_for_euclid_dist</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">nearest_indices</span> <span class="o">=</span> <span class="n">distances</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">nearest_indices</span><span class="p">]</span>


<span class="c1"># model wrapper for the solvers
</span><span class="k">class</span> <span class="nc">ModelWrapper</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This class is used to wrap around another model, adding custom forward pass logic.
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">)</span>
    
    

<span class="k">class</span> <span class="nc">Solver</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Abstract base class for solvers.</span><span class="sh">"""</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">pass</span>

        
<span class="k">class</span> <span class="nc">MixtureDiscreteSolver</span><span class="p">(</span><span class="n">Solver</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ModelWrapper</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">MixtureDiscreteProbPath</span><span class="p">,</span>
        <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">source_distribution_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">solver_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span>
    <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocabulary_size</span>

        <span class="k">if</span> <span class="n">source_distribution_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">source_distribution_p</span><span class="p">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nc">Size</span><span class="p">(</span>
                <span class="p">[</span><span class="n">vocabulary_size</span><span class="p">]</span>
            <span class="p">),</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Source distribution p dimension must match the vocabulary size </span><span class="si">{</span><span class="n">vocabulary_size</span><span class="si">}</span><span class="s">. Got </span><span class="si">{</span><span class="n">source_distribution_p</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">source_distribution_p</span> <span class="o">=</span> <span class="n">source_distribution_p</span>
        
        <span class="k">assert</span> <span class="n">solver_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">solver_type</span> <span class="o">=</span> <span class="n">solver_type</span>

    <span class="nd">@torch.no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x_init</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">step_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">div_free</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">float</span><span class="p">],</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">dtype_categorical</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">time_grid</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span>
        <span class="n">return_intermediates</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_extras</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="n">div_free</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="nf">assert </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">source_distribution_p</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">),</span> <span class="sh">"</span><span class="s">Source distribution p must be specified in order to add a divergence-free term to the probability velocity.</span><span class="sh">"</span>

        <span class="c1"># Initialize the current state `x_t` with the initial state `X_0`.
</span>        <span class="n">time_grid</span> <span class="o">=</span> <span class="n">time_grid</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">x_init</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">step_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># If step_size is None then set the t discretization to time_grid.
</span>            <span class="n">t_discretization</span> <span class="o">=</span> <span class="n">time_grid</span>
            <span class="n">n_steps</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">time_grid</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If step_size is float then t discretization is uniform with step size set by step_size.
</span>            <span class="n">t_init</span> <span class="o">=</span> <span class="n">time_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">t_final</span> <span class="o">=</span> <span class="n">time_grid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
            <span class="nf">assert </span><span class="p">(</span><span class="n">t_final</span> <span class="o">-</span> <span class="n">t_init</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">step_size</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Time interval [time_grid[0], time_grid[-1]] must be larger than step_size. Got a time interval [</span><span class="si">{</span><span class="n">t_init</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">t_final</span><span class="si">}</span><span class="s">] and step_size </span><span class="si">{</span><span class="n">step_size</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span>

            <span class="n">n_steps</span> <span class="o">=</span> <span class="nf">ceil</span><span class="p">((</span><span class="n">t_final</span> <span class="o">-</span> <span class="n">t_init</span><span class="p">)</span> <span class="o">/</span> <span class="n">step_size</span><span class="p">)</span>
            <span class="n">t_discretization</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">t_init</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">t_final</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x_init</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_intermediates</span><span class="p">:</span>
                <span class="c1"># get order of intermediate steps:
</span>                <span class="n">order</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">time_grid</span><span class="p">)</span>
                <span class="c1"># Compute intermediate steps to return via nearest points in t_discretization to time_grid.
</span>                <span class="n">time_grid</span> <span class="o">=</span> <span class="nf">get_nearest_times</span><span class="p">(</span><span class="n">time_grid</span><span class="o">=</span><span class="n">time_grid</span><span class="p">,</span> <span class="n">t_discretization</span><span class="o">=</span><span class="n">t_discretization</span><span class="p">)</span>

        <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_init</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
        <span class="n">steps_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">return_intermediates</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_init</span><span class="p">.</span><span class="nf">clone</span><span class="p">()]</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">ctx</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">t_final</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">NFE: </span><span class="si">{</span><span class="n">steps_counter</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ctx</span> <span class="o">=</span> <span class="nf">nullcontext</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

                <span class="c1"># Sample x_1 ~ p_1|t( \cdot |x_t)
</span>                <span class="n">p_1t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">model_extras</span><span class="p">)</span>
                <span class="n">x_1</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">p_1t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>

                <span class="c1"># Checks if final step
</span>                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">n_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Compute u_t(x|x_t,x_1)
</span>                    <span class="n">scheduler_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>

                    <span class="n">k_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">alpha_t</span>
                    <span class="n">d_k_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">d_alpha_t</span>

                    <span class="n">delta_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">k_t</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">u</span> <span class="o">=</span> <span class="n">d_k_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta_1</span>

                    <span class="c1"># Add divergence-free part
</span>                    <span class="n">div_free_t</span> <span class="o">=</span> <span class="nf">div_free</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">if</span> <span class="nf">callable</span><span class="p">(</span><span class="n">div_free</span><span class="p">)</span> <span class="k">else</span> <span class="n">div_free</span>

                    <span class="k">if</span> <span class="n">div_free_t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">p_0</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">source_distribution_p</span><span class="p">[(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">*</span> <span class="n">x_t</span><span class="p">.</span><span class="nf">dim</span><span class="p">()]</span>
                        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span> <span class="o">+</span> <span class="n">div_free_t</span> <span class="o">*</span> <span class="n">d_k_t</span> <span class="o">/</span> <span class="p">(</span><span class="n">k_t</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_t</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">p_0</span> <span class="o">+</span> <span class="n">k_t</span> <span class="o">*</span> <span class="n">delta_1</span><span class="p">)</span>

                    <span class="c1"># Set u_t(x_t|x_t,x_1) = 0
</span>                    <span class="n">delta_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                    <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">delta_t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">u</span><span class="p">),</span> <span class="n">u</span><span class="p">)</span>

                    <span class="c1"># Sample x_t ~ u_t( \cdot |x_t,x_1) -- predictor
</span>                    <span class="n">intensity</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Assuming u_t(xt|xt,x1) := 0
</span>                    <span class="n">mask_jump</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x_t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">h</span> <span class="o">*</span> <span class="n">intensity</span><span class="p">)</span>

                    <span class="k">if</span> <span class="n">mask_jump</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">x_t</span><span class="p">[</span><span class="n">mask_jump</span><span class="p">]</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="n">mask_jump</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>
                        
                    <span class="c1">#### the following is only for Heun method
</span>                    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">solver_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">:</span>
                        <span class="n">x_th</span> <span class="o">=</span> <span class="n">x_t</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
                        <span class="n">th</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">h</span>
                        <span class="n">p_1th</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_th</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">th</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">x_th</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">model_extras</span><span class="p">)</span>
                        <span class="n">x_1th</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">p_1th</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>

                        <span class="n">scheduler_output_th</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">th</span><span class="p">)</span>

                        <span class="n">k_th</span> <span class="o">=</span> <span class="n">scheduler_output_th</span><span class="p">.</span><span class="n">alpha_t</span>
                        <span class="n">d_k_th</span> <span class="o">=</span> <span class="n">scheduler_output_th</span><span class="p">.</span><span class="n">d_alpha_t</span>

                        <span class="n">delta_1th</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_1th</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">k_th</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
                        <span class="n">u_th</span> <span class="o">=</span> <span class="n">d_k_th</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_th</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta_1th</span>

                        <span class="c1"># Add divergence-free part
</span>                        <span class="n">div_free_th</span> <span class="o">=</span> <span class="nf">div_free</span><span class="p">(</span><span class="n">th</span><span class="p">)</span> <span class="k">if</span> <span class="nf">callable</span><span class="p">(</span><span class="n">div_free</span><span class="p">)</span> <span class="k">else</span> <span class="n">div_free</span>

                        <span class="k">if</span> <span class="n">div_free_th</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">p_0</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">source_distribution_p</span><span class="p">[(</span><span class="bp">None</span><span class="p">,)</span> <span class="o">*</span> <span class="n">x_th</span><span class="p">.</span><span class="nf">dim</span><span class="p">()]</span>
                            <span class="n">u_th</span> <span class="o">=</span> <span class="n">u_th</span> <span class="o">+</span> <span class="n">div_free_th</span> <span class="o">*</span> <span class="n">d_k_th</span> <span class="o">/</span> <span class="p">(</span><span class="n">k_th</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_th</span><span class="p">))</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_th</span><span class="p">)</span> <span class="o">*</span> <span class="n">p_0</span> <span class="o">+</span> <span class="n">k_th</span> <span class="o">*</span> <span class="n">delta_1th</span><span class="p">)</span>

                        <span class="c1"># Set u_t(x_t|x_t,x_1) = 0
</span>                        <span class="n">delta_th</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_th</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                        <span class="n">u_th</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">delta_th</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">bool</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">u_th</span><span class="p">),</span> <span class="n">u_th</span><span class="p">)</span>

                        <span class="c1"># combine u and u_{t+h} -- corrector
</span>                        <span class="n">u</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">u</span> <span class="o">+</span> <span class="n">u_th</span><span class="p">)</span>
                        <span class="n">intensity</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">mask_jump</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x_t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">h</span> <span class="o">*</span> <span class="n">intensity</span><span class="p">)</span>
                        <span class="k">if</span> <span class="n">mask_jump</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">x_t</span><span class="p">[</span><span class="n">mask_jump</span><span class="p">]</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">u</span><span class="p">[</span><span class="n">mask_jump</span><span class="p">].</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>
                
                <span class="n">steps_counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">h</span>

                <span class="k">if</span> <span class="n">return_intermediates</span> <span class="ow">and</span> <span class="p">(</span><span class="n">t</span> <span class="ow">in</span> <span class="n">time_grid</span><span class="p">):</span>
                    <span class="n">res</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="nf">clone</span><span class="p">())</span>

                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="n">ctx</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                    <span class="n">ctx</span><span class="p">.</span><span class="nf">refresh</span><span class="p">()</span>
                    <span class="n">ctx</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NFE: </span><span class="si">{</span><span class="n">steps_counter</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_intermediates</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">step_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_t</span>
        
        
<span class="k">class</span> <span class="nc">SimpleSolver</span><span class="p">(</span><span class="n">Solver</span><span class="p">):</span> 
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ModelWrapper</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">MixtureDiscreteProbPath</span><span class="p">,</span>
        <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
        <span class="n">solver_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">stochastic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">source_distribution</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span>
    <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocabulary_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">solver_type</span> <span class="o">=</span> <span class="n">solver_type</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stochastic</span> <span class="o">=</span> <span class="n">stochastic</span>
        <span class="n">self</span><span class="p">.</span><span class="n">source_distribution</span> <span class="o">=</span> <span class="n">source_distribution</span>
        
    <span class="nd">@torch.no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">x_init</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">step_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
        <span class="n">dtype_categorical</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
        <span class="n">time_grid</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span>
        <span class="n">return_intermediates</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">model_extras</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        
        <span class="c1"># Initialize the current state `x_t` with the initial state `X_0`.
</span>        <span class="n">time_grid</span> <span class="o">=</span> <span class="n">time_grid</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">x_init</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">step_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1"># If step_size is None then set the t discretization to time_grid.
</span>            <span class="n">t_discretization</span> <span class="o">=</span> <span class="n">time_grid</span>
            <span class="n">n_steps</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">time_grid</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If step_size is float then t discretization is uniform with step size set by step_size.
</span>            <span class="n">t_init</span> <span class="o">=</span> <span class="n">time_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">t_final</span> <span class="o">=</span> <span class="n">time_grid</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span>
            <span class="nf">assert </span><span class="p">(</span><span class="n">t_final</span> <span class="o">-</span> <span class="n">t_init</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">step_size</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Time interval [time_grid[0], time_grid[-1]] must be larger than step_size. Got a time interval [</span><span class="si">{</span><span class="n">t_init</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">t_final</span><span class="si">}</span><span class="s">] and step_size </span><span class="si">{</span><span class="n">step_size</span><span class="si">}</span><span class="s">.</span><span class="sh">"</span>

            <span class="n">n_steps</span> <span class="o">=</span> <span class="nf">ceil</span><span class="p">((</span><span class="n">t_final</span> <span class="o">-</span> <span class="n">t_init</span><span class="p">)</span> <span class="o">/</span> <span class="n">step_size</span><span class="p">)</span>
            <span class="n">t_discretization</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">t_init</span> <span class="o">+</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="n">t_final</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x_init</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">return_intermediates</span><span class="p">:</span>
                <span class="c1"># get order of intermediate steps:
</span>                <span class="n">order</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">time_grid</span><span class="p">)</span>
                <span class="c1"># Compute intermediate steps to return via nearest points in t_discretization to time_grid.
</span>                <span class="n">time_grid</span> <span class="o">=</span> <span class="nf">get_nearest_times</span><span class="p">(</span><span class="n">time_grid</span><span class="o">=</span><span class="n">time_grid</span><span class="p">,</span> <span class="n">t_discretization</span><span class="o">=</span><span class="n">t_discretization</span><span class="p">)</span>

        <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_init</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
        <span class="n">steps_counter</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">res</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="n">return_intermediates</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_init</span><span class="p">.</span><span class="nf">clone</span><span class="p">()]</span>

        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
            <span class="n">ctx</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">total</span><span class="o">=</span><span class="n">t_final</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">NFE: </span><span class="si">{</span><span class="n">steps_counter</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ctx</span> <span class="o">=</span> <span class="nf">nullcontext</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">ctx</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="n">h</span> <span class="o">=</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">t_discretization</span><span class="p">[</span><span class="n">i</span> <span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

                <span class="c1"># Sample x_1 ~ p_1|t( \cdot |x_t)
</span>                <span class="n">p_1t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">model_extras</span><span class="p">)</span>
                <span class="n">x_1</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">p_1t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>

                <span class="c1"># Checks if final step
</span>                <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">n_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="n">x_t</span> <span class="o">=</span> <span class="n">x_1</span>
                <span class="k">else</span><span class="p">:</span>
                    
                    <span class="c1"># kappa
</span>                    <span class="n">scheduler_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
                    <span class="n">k_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">alpha_t</span>
                    <span class="n">d_k_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">d_alpha_t</span>
                    
                    <span class="c1"># PMFs
</span>                    <span class="n">p_1t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">p_1t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">delta_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                    <span class="n">delta_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                    
                    <span class="c1"># velocity
</span>                    <span class="n">u_t</span> <span class="o">=</span> <span class="n">d_k_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta_1</span> <span class="c1">#+ s_t * delta_n
</span>                    
                    <span class="c1"># Euler point
</span>                    <span class="n">p_t</span> <span class="o">=</span> <span class="n">delta_t</span> <span class="o">+</span> <span class="n">h</span> <span class="o">*</span> <span class="n">u_t</span>
                    
                    <span class="c1">###  Start Heun
</span>                    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">solver_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">:</span> 
                        <span class="n">x_th</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">p_t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>
                    
                        <span class="n">th</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">h</span>
                        <span class="n">p_1th</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_th</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">th</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="o">**</span><span class="n">model_extras</span><span class="p">)</span>
                        <span class="n">x_1th</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">p_1th</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>

                        <span class="c1"># kappa
</span>                        <span class="n">scheduler_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">scheduler</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">th</span><span class="p">)</span>
                        <span class="n">k_th</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">alpha_t</span>
                        <span class="n">d_k_th</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">d_alpha_t</span>

                        <span class="c1"># PMFs
</span>                        <span class="n">p_1t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">p_1th</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
                        <span class="n">delta_th</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_th</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                        <span class="n">delta_1th</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_1th</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                        <span class="n">u_th</span> <span class="o">=</span> <span class="n">d_k_th</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k_th</span><span class="p">)</span> <span class="o">*</span> <span class="n">delta_1th</span>

                        <span class="c1"># Heun
</span>                        <span class="n">p_t</span> <span class="o">=</span> <span class="n">delta_t</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="n">u_t</span> <span class="o">+</span> <span class="n">u_th</span><span class="p">)</span>
                    
                    <span class="c1">### Start stochastic
</span>                    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">stochastic</span><span class="p">:</span> 
                        <span class="c1"># noise PMFs with uniform
</span>                        <span class="n">s_t</span> <span class="o">=</span> <span class="n">scheduler_output</span><span class="p">.</span><span class="n">sigma_t</span>
                        
                        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">:</span>
                            <span class="n">x_n</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                        <span class="k">elif</span> <span class="n">self</span><span class="p">.</span><span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span> 
                            <span class="n">x_n</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
                        
                        <span class="n">delta_n</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">x_n</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
                        <span class="n">p_t</span> <span class="o">+=</span> <span class="n">delta_n</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">s_t</span> <span class="o">**</span> <span class="mf">0.5</span>
                        
                    <span class="c1"># Sample
</span>                    <span class="n">x_t</span> <span class="o">=</span> <span class="nf">categorical</span><span class="p">(</span><span class="n">p_t</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype_categorical</span><span class="p">))</span>
                
                <span class="n">steps_counter</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">+</span> <span class="n">h</span>

                <span class="k">if</span> <span class="n">return_intermediates</span> <span class="ow">and</span> <span class="p">(</span><span class="n">t</span> <span class="ow">in</span> <span class="n">time_grid</span><span class="p">):</span>
                    <span class="n">res</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="nf">clone</span><span class="p">())</span>

                <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                    <span class="n">ctx</span><span class="p">.</span><span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
                    <span class="n">ctx</span><span class="p">.</span><span class="nf">refresh</span><span class="p">()</span>
                    <span class="n">ctx</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NFE: </span><span class="si">{</span><span class="n">steps_counter</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">return_intermediates</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">step_size</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="n">order</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x_t</span>
        
</code></pre></div></div> <h2 id="2-training-pipeline">2. Training Pipeline</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_discrete_flow_matching_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">x_1_gen_fn</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> <span class="n">source_distribution</span><span class="o">=</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="n">model_prefix</span><span class="o">=</span><span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">,</span> 
                                       <span class="n">vocab_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">):</span> 
    
    <span class="k">assert</span> <span class="n">loss_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">model_prefix</span><span class="si">}</span><span class="s"> training </span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s"> loss with </span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s"> distribution</span><span class="sh">'</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">model_prefix</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s">_it_</span><span class="si">{</span><span class="n">iterations</span><span class="si">:</span><span class="mi">06</span><span class="n">d</span><span class="si">}</span><span class="sh">'</span>
    
    <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">added_token</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">mask_token</span> <span class="o">=</span> <span class="n">vocab_size</span>  <span class="c1"># tokens starting from zero
</span>        <span class="n">added_token</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="c1"># additional mask token
</span>    <span class="n">vocab_size</span> <span class="o">+=</span> <span class="n">added_token</span>

    <span class="c1"># model
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="c1"># optimizer
</span>    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    
    <span class="c1"># loss function
</span>    <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">:</span> 
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">:</span> 
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
        
    <span class="c1"># training loop
</span>    <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">pbar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>
        
        <span class="c1"># sample data x_1
</span>        <span class="n">x_1</span> <span class="o">=</span> <span class="nf">x_1_gen_fn</span><span class="p">(</span><span class="n">n_grid_points</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">-</span> <span class="n">added_token</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="c1"># sample data
</span>        
        <span class="c1"># sample noise x_0
</span>        <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask_token</span>

        <span class="c1"># sample time 
</span>        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>

        <span class="c1"># sample probability path, in this case, (X_0,X_1) ~ pi(X_0,X_1) = p_0(X_0)p_1(X_1)
</span>        <span class="n">path_sample</span> <span class="o">=</span> <span class="n">path</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">)</span>

        <span class="c1"># The model predicts the logits for x_1 given x_t and t
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">path_sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">path_sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
        
        <span class="c1"># discrete flow matching generalized KL loss or Cross Entropy loss
</span>        <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">:</span> 
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">path_sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">path_sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">:</span> 
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">einops</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="sh">'</span><span class="s">b n d -&gt; (b n) d</span><span class="sh">'</span><span class="p">),</span> 
                           <span class="n">einops</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="sh">'</span><span class="s">b n -&gt; (b n)</span><span class="sh">'</span><span class="p">))</span>

        <span class="c1"># optimizer step
</span>        <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
        <span class="n">optim</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

        <span class="c1"># loss logging
</span>        <span class="n">losses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
        <span class="n">pbar</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">, Loss = </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="mf">2.3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># checkpoint
</span>    <span class="n">savepath</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">SAVE_PATH</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s">.pth</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> 
                  <span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">losses</span><span class="p">)}</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">savepath</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">losses</span>
    
</code></pre></div></div> <h2 id="3-elbo-estimates">3. ELBO Estimates</h2> <p>The generalized KL divergence is also used to compute ELBO estimates:</p> \[\log{p_1(x_1)} \geq -\sum_i \textbf{E}_{t, (X_0, X_1), X_t}\frac{\dot{\kappa_t}}{1-\kappa_t}\left[(\delta_{x_1}(x_t^i) - 1)\log{p_{1 \mid t}(x_1^i \mid x_t)} + \delta_{x_1}(x_t^i) - p_{1 \mid t}(x_1^i \mid x_t) \right]\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># elbo estimate given any x_1 in the domain but not necessarity in the data distribution
</span><span class="k">def</span> <span class="nf">elbo_estimate</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">source_distribution</span><span class="p">,</span> <span class="n">n_discretization</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">):</span> 
    
    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">x_1</span><span class="p">.</span><span class="n">device</span> <span class="o">==</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">x_1</span><span class="p">.</span><span class="n">device</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="n">generalized_kl_fn</span> <span class="o">=</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span><span class="p">,</span> <span class="n">reduction</span> <span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">discretization</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_discretization</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="n">elbo</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)):</span>
            <span class="c1"># Lower variance estimator for time discretization
</span>            <span class="n">discretization</span> <span class="o">=</span> <span class="n">discretization</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">discretization</span> <span class="o">=</span> <span class="n">discretization</span> <span class="o">%</span> <span class="mi">1</span>
            <span class="n">discretization</span> <span class="o">=</span> <span class="n">discretization</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-4</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">discretization</span><span class="p">:</span>
                <span class="c1"># sample X_t ~ p_t(\cdot| x_1)
</span>                <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">:</span>
                    <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span>
                    <span class="n">x_0</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="n">vocab_size</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
                <span class="n">x_t</span> <span class="o">=</span> <span class="n">path</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">).</span><span class="n">x_t</span>

                <span class="n">logits</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

                <span class="c1"># compute ELBO
</span>                <span class="n">elbo</span> <span class="o">-=</span> <span class="nf">generalized_kl_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">elbo</span> <span class="o">/=</span> <span class="n">n_discretization</span> <span class="o">*</span> <span class="n">n_samples</span>

    <span class="c1"># Remember that log_q(x_1) &gt;= ELBO(x_1)
</span>    <span class="n">probability_lower_bound</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">elbo</span><span class="p">)</span>
    <span class="n">log_prob_lower_bound_per_dim</span> <span class="o">=</span> <span class="n">elbo</span> <span class="o">/</span> <span class="n">dim</span>
    
    <span class="k">return</span> <span class="p">{</span><span class="sh">'</span><span class="s">elbo</span><span class="sh">'</span><span class="p">:</span> <span class="n">probability_lower_bound</span><span class="p">,</span> 
            <span class="sh">'</span><span class="s">logp_per_dim</span><span class="sh">'</span><span class="p">:</span> <span class="n">log_prob_lower_bound_per_dim</span><span class="p">}</span>
</code></pre></div></div> <h1 id="2d-case">2D Case</h1> <p>Here, we generate a 2D checkerboard data as \(x_1\) and train discrete flow matching model to replicate this discrete distributions.</p> <p>The model is a simple MLP predicting the logits for each position.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inf_train_gen_2d</span><span class="p">(</span><span class="n">n_grid_points</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">n_grid_points</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">number of grid points has to be divisible by 4</span><span class="sh">"</span>
    
    <span class="n">n_grid_points</span> <span class="o">=</span> <span class="n">n_grid_points</span> <span class="o">//</span> <span class="mi">4</span>
    
    <span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">n_grid_points</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">samples_x2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">n_grid_points</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">x2</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">samples_x2</span>
        <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_grid_points</span>
        <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_grid_points</span>
        <span class="o">+</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">floor</span><span class="p">(</span><span class="n">x1</span> <span class="o">/</span> <span class="n">n_grid_points</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_grid_points</span>
    <span class="p">)</span>
    
    <span class="n">x_end</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x1</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">x2</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_end</span><span class="p">.</span><span class="nf">long</span><span class="p">()</span>

<span class="c1"># Activation class
</span><span class="k">class</span> <span class="nc">Swish</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span> 
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>

<span class="c1"># Model class
</span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_dim</span> <span class="o">=</span> <span class="n">time_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>

        <span class="n">self</span><span class="p">.</span><span class="n">time_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">token_embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">main</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="nc">Swish</span><span class="p">(),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="n">length</span> <span class="o">+</span> <span class="n">time_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="nc">Swish</span><span class="p">(),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="nc">Swish</span><span class="p">(),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="nc">Swish</span><span class="p">(),</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">input_dim</span> <span class="o">*</span> <span class="n">length</span><span class="p">),</span>
        <span class="p">)</span>
        

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">time_embedding</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">token_embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span> <span class="o">*</span> <span class="n">d</span><span class="p">)</span>
        
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">main</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">input_dim</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h</span>
    
    
<span class="k">class</span> <span class="nc">WrappedModel</span><span class="p">(</span><span class="n">ModelWrapper</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">extras</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># hyperparams
</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">30000</span>

<span class="c1"># scheduler definition
</span><span class="n">scheduler</span> <span class="o">=</span> <span class="nc">PolynomialScheduler</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">path</span> <span class="o">=</span> <span class="nc">MixtureDiscreteProbPath</span><span class="p">(</span><span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>

<span class="c1"># do a sweep for 
# 1. source = ['uniform', 'mask']
# 2. loss = ['KL', 'CE']
# {model, losses, samples}
# samples are dict of different type
</span>
<span class="n">rlt_dct</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>

<span class="k">for</span> <span class="n">source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]:</span>
    
    <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">]</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">loss_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">]:</span>
        <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">]</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
        
        <span class="n">input_dim</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="nf">int</span><span class="p">(</span><span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">losses</span> <span class="o">=</span> <span class="nf">train_discrete_flow_matching_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">inf_train_gen_2d</span><span class="p">,</span> 
                                                           <span class="n">loss_type</span><span class="o">=</span><span class="n">loss_type</span><span class="p">,</span> 
                                                           <span class="n">source_distribution</span><span class="o">=</span><span class="n">source_distribution</span><span class="p">,</span> 
                                                           <span class="n">model_prefix</span><span class="o">=</span><span class="sh">'</span><span class="s">twodim</span><span class="sh">'</span><span class="p">,</span> 
                                                           <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> 
                                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                                           <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> 
                                                           <span class="n">iterations</span><span class="o">=</span><span class="n">iterations</span><span class="p">,</span> 
                                                           <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> 
                                                           <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="sh">'</span><span class="s">losses</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
    
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># training a larger scoring model using the same scheduler: 
</span>
<span class="n">sc_hidden_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">sc_iterations</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="n">scoring_model</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
<span class="k">for</span> <span class="n">sc_source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]:</span>
    
    <span class="n">sc_input_dim</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="nf">int</span><span class="p">(</span><span class="n">sc_source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">sc_model</span> <span class="o">=</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">sc_input_dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="n">sc_hidden_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">sc_model</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="nf">train_discrete_flow_matching_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">inf_train_gen_2d</span><span class="p">,</span> 
                                                      <span class="n">loss_type</span><span class="o">=</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> 
                                                      <span class="n">source_distribution</span><span class="o">=</span><span class="n">sc_source_distribution</span><span class="p">,</span> 
                                                      <span class="n">model_prefix</span><span class="o">=</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span> 
                                                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> 
                                                      <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
                                                      <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> 
                                                      <span class="n">iterations</span><span class="o">=</span><span class="n">sc_iterations</span><span class="p">,</span> 
                                                      <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> 
                                                      <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="n">scoring_model</span><span class="p">[</span><span class="n">sc_source_distribution</span><span class="p">]</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">sc_model</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># sampling function
</span><span class="k">def</span> <span class="nf">sample_discrete_flow_matching_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> 
                                        <span class="n">source_distribution</span><span class="o">=</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="n">solver_type</span><span class="o">=</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000000</span><span class="p">,</span> <span class="n">nfe</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_plots</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
    
    <span class="k">assert</span> <span class="n">source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">solver_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEuler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEulerStochastic</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeunStochastic</span><span class="sh">'</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Sampling with </span><span class="si">{</span><span class="n">solver_type</span><span class="si">}</span><span class="s"> solver</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># infer device
</span>    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>
    
    <span class="c1"># model wrapper
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">wrapped_model</span> <span class="o">=</span> <span class="nc">WrappedModel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">solver_type</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">Simple</span><span class="sh">'</span><span class="p">):</span> 
        <span class="n">solver</span> <span class="o">=</span> <span class="nc">SimpleSolver</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">wrapped_model</span><span class="p">,</span> 
                              <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> 
                              <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="nf">int</span><span class="p">(</span><span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">),</span> 
                              <span class="n">solver_type</span><span class="o">=</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span> <span class="k">if</span> <span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">solver_type</span> <span class="k">else</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">,</span> 
                              <span class="n">stochastic</span><span class="o">=</span><span class="sh">'</span><span class="s">Stochastic</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">solver_type</span><span class="p">,</span> 
                              <span class="n">source_distribution</span><span class="o">=</span><span class="n">source_distribution</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">solver</span> <span class="o">=</span> <span class="nc">MixtureDiscreteSolver</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">wrapped_model</span><span class="p">,</span> 
                                       <span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> 
                                       <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="nf">int</span><span class="p">(</span><span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">),</span> 
                                       <span class="n">solver_type</span><span class="o">=</span><span class="n">solver_type</span><span class="p">)</span>
    
    <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">nfe</span>
    
    <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">:</span> 
        <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span> 
        <span class="n">x_0</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="n">vocab_size</span><span class="p">).</span><span class="nf">long</span><span class="p">()</span>
    
    <span class="n">linspace_to_plot</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">n_plots</span><span class="p">)</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="n">solver</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">x_init</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> 
                        <span class="n">step_size</span><span class="o">=</span><span class="n">step_size</span><span class="p">,</span> 
                        <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                        <span class="n">return_intermediates</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">time_grid</span><span class="o">=</span><span class="n">linspace_to_plot</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sol</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">plot_2d_sol</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span> 
    <span class="n">n_plots</span> <span class="o">=</span> <span class="n">sol</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">linspace_to_plot</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">n_plots</span><span class="p">)</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_plots</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">mask_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">]).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">linspace_to_plot</span><span class="p">):</span>
        
        <span class="n">sol_step</span> <span class="o">=</span> <span class="n">sol</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">...]</span>
        <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">sol_step</span> <span class="o">=</span> <span class="n">sol_step</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">ne</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">sol_step</span><span class="p">),</span> <span class="n">mask_tensor</span><span class="p">).</span><span class="nf">all</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="p">...]</span>

        <span class="n">H</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">hist2d</span><span class="p">(</span><span class="n">sol_step</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sol_step</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>

        <span class="n">cmin</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">cmax</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">quantile</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">H</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="mf">0.95</span><span class="p">).</span><span class="nf">item</span><span class="p">()</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="n">colors</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">vmax</span><span class="o">=</span><span class="n">cmax</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cmin</span><span class="p">)</span>
        <span class="n">_</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">hist2d</span><span class="p">(</span><span class="n">sol_step</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">sol_step</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>

        <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">'</span><span class="s">equal</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">t= </span><span class="si">{</span><span class="n">linspace_to_plot</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span> <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">title</span><span class="si">}</span><span class="s"> t= </span><span class="si">{</span><span class="n">linspace_to_plot</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">for</span> <span class="n">source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">loss_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">]:</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">solver_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEuler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEulerStochastic</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeunStochastic</span><span class="sh">'</span><span class="p">]:</span> 
            <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="n">solver_type</span><span class="p">]</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
            <span class="n">sol</span> <span class="o">=</span> <span class="nf">sample_discrete_flow_matching_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> 
                                                      <span class="n">source_distribution</span><span class="o">=</span><span class="n">source_distribution</span><span class="p">,</span> 
                                                      <span class="n">solver_type</span><span class="o">=</span><span class="n">solver_type</span><span class="p">,</span> 
                                                      <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            
            <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="n">solver_type</span><span class="p">][</span><span class="sh">'</span><span class="s">sample</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            
            <span class="c1"># randomly generate plots
</span>            <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.1</span><span class="p">:</span> <span class="n">fig</span> <span class="o">=</span> <span class="nf">plot_2d_sol</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">solver_type</span><span class="si">}</span><span class="se">\n</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/dfm/fig1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/dfm/fig1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/dfm/fig1-1400.webp"></source> <img src="/assets/img/posts/dfm/fig1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_2d_elbo</span><span class="p">(</span><span class="n">elbo_dct</span><span class="p">):</span> 
    <span class="n">probability_lower_bound</span> <span class="o">=</span> <span class="n">elbo_dct</span><span class="p">[</span><span class="sh">'</span><span class="s">elbo</span><span class="sh">'</span><span class="p">]</span>
    <span class="n">log_prob_lower_bound_per_dim</span> <span class="o">=</span> <span class="n">elbo_dct</span><span class="p">[</span><span class="sh">'</span><span class="s">logp_per_dim</span><span class="sh">'</span><span class="p">]</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

    <span class="n">cmin</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">cmax</span> <span class="o">=</span> <span class="n">probability_lower_bound</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span> <span class="o">/</span> <span class="mf">1.5</span> 
    <span class="n">norm</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="n">colors</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">vmax</span><span class="o">=</span><span class="n">cmax</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cmin</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">probability_lower_bound</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">).</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">origin</span><span class="o">=</span><span class="sh">'</span><span class="s">lower</span><span class="sh">'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">ELBO Estimator</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">cm</span><span class="p">.</span><span class="nc">ScalarMappable</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">orientation</span><span class="o">=</span><span class="sh">'</span><span class="s">horizontal</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">density</span><span class="sh">'</span><span class="p">)</span>


    <span class="n">cmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">8.0</span>
    <span class="n">cmax</span> <span class="o">=</span> <span class="o">-</span><span class="mf">3.0</span>
    <span class="n">norm</span> <span class="o">=</span> <span class="n">cm</span><span class="p">.</span><span class="n">colors</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">(</span><span class="n">vmax</span><span class="o">=</span><span class="n">cmax</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="n">cmin</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">log_prob_lower_bound_per_dim</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">).</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">origin</span><span class="o">=</span><span class="sh">'</span><span class="s">lower</span><span class="sh">'</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">logP(x_1)/dim Estimator</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">cm</span><span class="p">.</span><span class="nc">ScalarMappable</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">coolwarm</span><span class="sh">'</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">orientation</span><span class="o">=</span><span class="sh">'</span><span class="s">horizontal</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">density</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fig</span>


<span class="c1"># Grid of vocab_size X vocab_size
</span><span class="n">grid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
                      <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
                      <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">grid</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># using the last model, (mask, CE)
</span><span class="n">elbo_dct</span> <span class="o">=</span> <span class="nf">elbo_estimate</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">source_distribution</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="nf">plot_2d_elbo</span><span class="p">(</span><span class="n">elbo_dct</span><span class="p">)</span>

</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/dfm/fig2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/dfm/fig2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/dfm/fig2-1400.webp"></source> <img src="/assets/img/posts/dfm/fig2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># show and plot the elbo and logp/dim
</span><span class="k">for</span> <span class="n">source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">loss_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">]:</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="se">\n</span><span class="s">----- </span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s"> -----</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">solver_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEuler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEulerStochastic</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeunStochastic</span><span class="sh">'</span><span class="p">]:</span> 

            <span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="n">solver_type</span><span class="p">][</span><span class="sh">'</span><span class="s">sample</span><span class="sh">'</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">elbo_dct</span> <span class="o">=</span> <span class="nf">elbo_estimate</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">source_distribution</span><span class="p">)</span>
            <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="n">solver_type</span><span class="p">].</span><span class="nf">update</span><span class="p">(</span><span class="n">elbo_dct</span><span class="p">)</span>
            
            <span class="n">elbo_mean</span> <span class="o">=</span> <span class="n">elbo_dct</span><span class="p">[</span><span class="sh">'</span><span class="s">elbo</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">log_p_mean</span> <span class="o">=</span> <span class="n">elbo_dct</span><span class="p">[</span><span class="sh">'</span><span class="s">logp_per_dim</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">solver_type</span><span class="si">}</span><span class="s">, ELBO = </span><span class="si">{</span><span class="n">elbo_mean</span><span class="si">:</span><span class="p">.</span><span class="mi">8</span><span class="n">f</span><span class="si">}</span><span class="s">, LogP/dim = </span><span class="si">{</span><span class="n">log_p_mean</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="se">\n</span><span class="s">-----</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Uniform</p> <table border="1"> <thead> <tr> <th>Method</th> <th>ELBO (KL / CE)</th> <th>LogP/dim (KL / CE)</th> </tr> </thead> <tbody> <tr> <td>Euler</td> <td>0.00001383 / 0.00001103</td> <td>-5.62259 / -5.81091</td> </tr> <tr> <td>Heun</td> <td> <strong>0.00001418</strong> / 0.00001150</td> <td> <strong>-5.60660</strong> / -5.77359</td> </tr> <tr> <td>SimpleEuler</td> <td>0.00001381 / 0.00001101</td> <td>-5.62421 / -5.81599</td> </tr> <tr> <td>SimpleHeun</td> <td>0.00001383 / 0.00001107</td> <td>-5.62253 / -5.80796</td> </tr> <tr> <td>SimpleEulerStochastic</td> <td>0.00001374 / 0.00001094</td> <td>-5.63223 / -5.82770</td> </tr> <tr> <td>SimpleHeunStochastic</td> <td>0.00001376 / 0.00001101</td> <td>-5.63070 / -5.81804</td> </tr> </tbody> </table> <p>Mask</p> <table border="1"> <thead> <tr> <th>Method</th> <th>ELBO (KL / CE)</th> <th>LogP/dim (KL / CE)</th> </tr> </thead> <tbody> <tr> <td>Euler</td> <td>0.00012449 / 0.00012359</td> <td>-4.51112 / -4.51102</td> </tr> <tr> <td>Heun</td> <td>0.00012453 / <strong>0.00012368</strong> </td> <td>-4.50965 / <strong>-4.50959</strong> </td> </tr> <tr> <td>SimpleEuler</td> <td>0.00012445 / 0.00012361</td> <td>-4.51148 / -4.51128</td> </tr> <tr> <td>SimpleHeun</td> <td>0.00012451 / 0.00012360</td> <td>-4.51088 / -4.51113</td> </tr> <tr> <td>SimpleEulerStochastic</td> <td>0.00012452 / 0.00012361</td> <td>-4.51197 / -4.51254</td> </tr> <tr> <td>SimpleHeunStochastic</td> <td>0.00012451 / 0.00012360</td> <td>-4.51180 / -4.51192</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using the scoring model:
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Using the scoring model: </span><span class="sh">'</span><span class="p">)</span>

<span class="k">for</span> <span class="n">source_distribution</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">]:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">scoring_model</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">loss_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">]:</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="se">\n</span><span class="s">----- </span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s"> -----</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">solver_type</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">Euler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEuler</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeun</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleEulerStochastic</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SimpleHeunStochastic</span><span class="sh">'</span><span class="p">]:</span> 

            <span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="n">solver_type</span><span class="p">][</span><span class="sh">'</span><span class="s">sample</span><span class="sh">'</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">elbo_dct</span> <span class="o">=</span> <span class="nf">elbo_estimate</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">source_distribution</span><span class="p">)</span>
            <span class="n">rlt_dct</span><span class="p">[</span><span class="n">source_distribution</span><span class="p">][</span><span class="n">loss_type</span><span class="p">][</span><span class="n">solver_type</span><span class="p">].</span><span class="nf">update</span><span class="p">(</span><span class="n">elbo_dct</span><span class="p">)</span>
            
            <span class="n">elbo_mean</span> <span class="o">=</span> <span class="n">elbo_dct</span><span class="p">[</span><span class="sh">'</span><span class="s">elbo</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="n">log_p_mean</span> <span class="o">=</span> <span class="n">elbo_dct</span><span class="p">[</span><span class="sh">'</span><span class="s">logp_per_dim</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">solver_type</span><span class="si">}</span><span class="s">, ELBO = </span><span class="si">{</span><span class="n">elbo_mean</span><span class="si">:</span><span class="p">.</span><span class="mi">8</span><span class="n">f</span><span class="si">}</span><span class="s">, LogP/dim = </span><span class="si">{</span><span class="n">log_p_mean</span><span class="si">:</span><span class="p">.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="se">\n</span><span class="s">-----</span><span class="sh">'</span><span class="p">)</span>

</code></pre></div></div> <p>Uniform</p> <table border="1"> <thead> <tr> <th>Method</th> <th>ELBO (KL / CE)</th> <th>LogP/dim (KL / CE)</th> </tr> </thead> <tbody> <tr> <td>Euler</td> <td>0.00001458 / 0.00001428</td> <td>-5.59203 / -5.74789</td> </tr> <tr> <td>Heun</td> <td>0.00001462 / 0.00001437</td> <td> <strong>-5.58809</strong> / -5.70745</td> </tr> <tr> <td>SimpleEuler</td> <td>0.00001458 / 0.00001426</td> <td>-5.59285 / -5.75720</td> </tr> <tr> <td>SimpleHeun</td> <td>0.00001457 / 0.00001429</td> <td>-5.59236 / -5.74419</td> </tr> <tr> <td>SimpleEulerStochastic</td> <td>0.00001456 / 0.00001422</td> <td>-5.59908 / -5.77931</td> </tr> <tr> <td>SimpleHeunStochastic</td> <td>0.00001457 / 0.00001426</td> <td>-5.59803 / -5.76159</td> </tr> </tbody> </table> <p>Mask</p> <table border="1"> <thead> <tr> <th>Method</th> <th>ELBO (KL / CE)</th> <th>LogP/dim (KL / CE)</th> </tr> </thead> <tbody> <tr> <td>Euler</td> <td>0.00012271 / 0.00012283</td> <td>-4.51789 / -4.51597</td> </tr> <tr> <td>Heun</td> <td>0.00012271 / 0.00012290</td> <td>-4.51583 / <strong>-4.51402</strong> </td> </tr> <tr> <td>SimpleEuler</td> <td>0.00012268 / 0.00012283</td> <td>-4.51828 / -4.51631</td> </tr> <tr> <td>SimpleHeun</td> <td>0.00012272 / 0.00012287</td> <td>-4.51744 / -4.51593</td> </tr> <tr> <td>SimpleEulerStochastic</td> <td>0.00012270 / 0.00012280</td> <td>-4.51952 / -4.51832</td> </tr> <tr> <td>SimpleHeunStochastic</td> <td>0.00012269 / 0.00012284</td> <td>-4.51913 / -4.51717</td> </tr> </tbody> </table> <p><br></p> <h1 id="sequence">Sequence</h1> <p>Building on the previous source type, training loss and ssampling, we now stick to: [uniform-KL, mask-CE] + [Heun] for training a sequence generation model on <code class="language-plaintext highlighter-rouge">1hxe.a2m</code>, which is a MSA from Serine Protease (<code class="language-plaintext highlighter-rouge">PDB: 1HXE</code>).</p> <p>We used fixed length including gaps in the MSA, this enables easy data loading. However, one can train without fixed length data by grouping same-length data in a batch as \(x_1\) and sample noised version of \(x_t\). The sequence length varies from batch to batch, so does the compute (CPU/GPU/Mem). If there is one length being under-represented, one can sample more time point to compensate and get batch of the same size. This might need some massage in the dataloading and preprocessing time, which we don’t do here.</p> <p>The model backbone is a Discrete Diffusion Transformer (DDiT) module.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load and process msafile
</span><span class="n">RESTYPES</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">R</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">N</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">D</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Q</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">E</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">H</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">L</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">M</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">F</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">P</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">S</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">T</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">W</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">V</span><span class="sh">'</span><span class="p">]</span>
<span class="n">RESTYPES_WITH_X_GAP</span> <span class="o">=</span> <span class="n">RESTYPES</span> <span class="o">+</span> <span class="p">[</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">]</span>
<span class="n">RESTYPE_TO_IDX</span> <span class="o">=</span> <span class="p">{</span><span class="n">res</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">res</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">RESTYPES_WITH_X_GAP</span><span class="p">)}</span>

<span class="k">def</span> <span class="nf">msa_to_torch</span><span class="p">(</span><span class="n">msafile</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isfile</span><span class="p">(</span><span class="n">msafile</span><span class="p">)</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">msafile</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">msafile</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.a3m</span><span class="sh">'</span><span class="p">):</span> 
        <span class="n">seqs</span> <span class="o">=</span> <span class="p">[</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">a</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">line</span> <span class="k">if</span> <span class="n">a</span><span class="p">.</span><span class="nf">isupper</span><span class="p">()</span> <span class="ow">or</span> <span class="n">a</span> <span class="o">==</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">])</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span> <span class="nf">if </span><span class="p">(</span><span class="ow">not</span> <span class="n">line</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">#</span><span class="sh">'</span><span class="p">))</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">line</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">&gt;</span><span class="sh">'</span><span class="p">))]</span>
    <span class="k">elif</span> <span class="n">msafile</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.a2m</span><span class="sh">'</span><span class="p">):</span> 
        <span class="n">seqs</span><span class="p">,</span> <span class="n">tmp</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span> 
            <span class="k">if</span> <span class="n">line</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">&gt;</span><span class="sh">'</span><span class="p">):</span> 
                <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span> <span class="n">seqs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">tmp</span><span class="p">))</span>
                <span class="n">tmp</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">else</span><span class="p">:</span> 
                <span class="n">tmp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="nf">strip</span><span class="p">().</span><span class="nf">upper</span><span class="p">().</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">.</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">-</span><span class="sh">'</span><span class="p">))</span>

    <span class="n">nseq</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">seqs</span><span class="p">),</span> <span class="nf">len</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">nseq</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seq</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">enumerate</span><span class="p">(</span><span class="n">seqs</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">seq</span><span class="p">):</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">RESTYPE_TO_IDX</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">if</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">RESTYPE_TO_IDX</span> <span class="k">else</span> <span class="mi">20</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="k">def</span> <span class="nf">inf_seq_train_gen</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">nseq</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">device</span>
    <span class="k">assert</span> <span class="n">batch_size</span> <span class="o">&lt;=</span> <span class="n">nseq</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">nseq</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>



<span class="c1">## Model
# # model definition
</span><span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span> <span class="n">einops</span> <span class="kn">import</span> <span class="n">rearrange</span><span class="p">,</span> <span class="n">repeat</span>
<span class="kn">from</span> <span class="n">omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConf</span>
<span class="kn">from</span> <span class="n">omegaconf.dictconfig</span> <span class="kn">import</span> <span class="n">DictConfig</span>



<span class="k">class</span> <span class="nc">Rotary</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    From: https://github.com/louaaron/Score-Entropy-Discrete-Diffusion
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">base</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">base</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">"</span><span class="s">inv_freq</span><span class="sh">"</span><span class="p">,</span> <span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seq_len_cached</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cos_cached</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sin_cached</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">seq_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="n">seq_dim</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">seq_len</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">seq_len_cached</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">seq_len_cached</span> <span class="o">=</span> <span class="n">seq_len</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="n">seq_dim</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">type_as</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inv_freq</span><span class="p">)</span>
            <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">i,j-&gt;ij</span><span class="sh">"</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">inv_freq</span><span class="p">.</span><span class="nf">clone</span><span class="p">())</span>
            <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># dims are: batch, seq_len, qkv, head, dim
</span>            <span class="n">self</span><span class="p">.</span><span class="n">cos_cached</span> <span class="o">=</span> <span class="n">emb</span><span class="p">.</span><span class="nf">cos</span><span class="p">()[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:].</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">sin_cached</span> <span class="o">=</span> <span class="n">emb</span><span class="p">.</span><span class="nf">sin</span><span class="p">()[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:].</span><span class="nf">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># This makes the transformation on v an identity.
</span>            <span class="n">self</span><span class="p">.</span><span class="n">cos_cached</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:].</span><span class="nf">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">sin_cached</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:].</span><span class="nf">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">cos_cached</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">sin_cached</span>


<span class="k">def</span> <span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[...,</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">],</span> <span class="n">x</span><span class="p">[...,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>

    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">apply_rotary_emb_torch</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">interleaved</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    From: https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/layers/rotary.py#L20
    </span><span class="sh">"""</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:</span> <span class="n">cos</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:</span> <span class="n">sin</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>

    <span class="n">ro_dim</span> <span class="o">=</span> <span class="n">cos</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="k">assert</span> <span class="n">ro_dim</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span>
        <span class="n">cos</span><span class="p">,</span> <span class="sh">"</span><span class="s">... d -&gt; ... 1 (2 d)</span><span class="sh">"</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">interleaved</span> <span class="k">else</span> <span class="sh">"</span><span class="s">... d -&gt; ... 1 (d 2)</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="nf">repeat</span><span class="p">(</span>
        <span class="n">sin</span><span class="p">,</span> <span class="sh">"</span><span class="s">... d -&gt; ... 1 (2 d)</span><span class="sh">"</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">interleaved</span> <span class="k">else</span> <span class="sh">"</span><span class="s">... d -&gt; ... 1 (d 2)</span><span class="sh">"</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">[...,</span> <span class="p">:</span><span class="n">ro_dim</span><span class="p">]</span> <span class="o">*</span> <span class="n">cos</span> <span class="o">+</span> <span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">[...,</span> <span class="p">:</span><span class="n">ro_dim</span><span class="p">])</span> <span class="o">*</span> <span class="n">sin</span>


<span class="k">def</span> <span class="nf">bias_dropout_add_scale</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">residual</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">prob</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">scale</span> <span class="o">*</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">prob</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">modulate</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">shift</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">scale</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">scale</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>


<span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">([</span><span class="n">dim</span><span class="p">]))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">dim</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">weight</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>


<span class="k">class</span> <span class="nc">TimestepEmbedder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Embeds scalar timesteps into vector representations.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">frequency_embedding_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">frequency_embedding_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">SiLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">frequency_embedding_size</span> <span class="o">=</span> <span class="n">frequency_embedding_size</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">timestep_embedding</span><span class="p">(</span><span class="n">time</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_period</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        Create sinusoidal timestep embeddings.
        :param t: a 1-D Tensor of N indices, one per batch element.
                          These may be fractional.
        :param dim: the dimension of the output.
        :param max_period: controls the minimum frequency of the embeddings.
        :return: an (N, D) Tensor of positional embeddings.
        </span><span class="sh">"""</span>
        <span class="n">half</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span>
            <span class="o">-</span><span class="n">math</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">max_period</span><span class="p">)</span>
            <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="n">half</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="o">/</span> <span class="n">half</span>
        <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">time</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">freqs</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">args</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">args</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dim</span> <span class="o">%</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">embedding</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">embedding</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">embedding</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">time</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">t_freq</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">timestep_embedding</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">time</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">frequency_embedding_size</span><span class="p">)</span>
        <span class="n">t_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">t_freq</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t_emb</span>


<span class="k">class</span> <span class="nc">DDiTBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cond_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">mlp_ratio</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">dim</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">dim must be devisable by n_heads</span><span class="sh">"</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span>

        <span class="n">self</span><span class="p">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">qw</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">kw</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vw</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">attn_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">mlp_ratio</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">GELU</span><span class="p">(</span><span class="n">approximate</span><span class="o">=</span><span class="sh">"</span><span class="s">tanh</span><span class="sh">"</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">mlp_ratio</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">adaLN_modulation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">cond_dim</span><span class="p">,</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adaLN_modulation</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adaLN_modulation</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">rotary_cos_sin</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="p">(</span>
            <span class="n">shift_msa</span><span class="p">,</span>
            <span class="n">scale_msa</span><span class="p">,</span>
            <span class="n">gate_msa</span><span class="p">,</span>
            <span class="n">shift_mlp</span><span class="p">,</span>
            <span class="n">scale_mlp</span><span class="p">,</span>
            <span class="n">gate_mlp</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">adaLN_modulation</span><span class="p">(</span><span class="n">c</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">x_skip</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">modulate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">shift</span><span class="o">=</span><span class="n">shift_msa</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_msa</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">kw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">vw</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">item</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">enabled</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
            <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">rotary_cos_sin</span>
            <span class="n">original_dtype</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">dtype</span>

            <span class="n">q</span> <span class="o">=</span> <span class="nf">apply_rotary_emb_torch</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">q</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">cos</span><span class="o">=</span><span class="n">cos</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">sin</span><span class="o">=</span><span class="n">sin</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
            <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>
            <span class="n">k</span> <span class="o">=</span> <span class="nf">apply_rotary_emb_torch</span><span class="p">(</span>
                <span class="n">x</span><span class="o">=</span><span class="n">k</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">cos</span><span class="o">=</span><span class="n">cos</span><span class="p">.</span><span class="nf">float</span><span class="p">(),</span> <span class="n">sin</span><span class="o">=</span><span class="n">sin</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
            <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">original_dtype</span><span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="n">item</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">scaled_dot_product_attention</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b h s d -&gt; b s (h d)</span><span class="sh">"</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">bias_dropout_add_scale</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">attn_out</span><span class="p">(</span><span class="n">x</span><span class="p">),</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">gate_msa</span><span class="p">,</span>
            <span class="n">residual</span><span class="o">=</span><span class="n">x_skip</span><span class="p">,</span>
            <span class="n">prob</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">bias_dropout_add_scale</span><span class="p">(</span>
            <span class="n">x</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="nf">modulate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">shift</span><span class="o">=</span><span class="n">shift_mlp</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale_mlp</span><span class="p">)),</span>
            <span class="n">scale</span><span class="o">=</span><span class="n">gate_mlp</span><span class="p">,</span>
            <span class="n">residual</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>
            <span class="n">prob</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">DDitFinalLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">cond_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">norm_final</span> <span class="o">=</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">adaLN_modulation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">cond_dim</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adaLN_modulation</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">adaLN_modulation</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">zero_</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">c</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">shift</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">adaLN_modulation</span><span class="p">(</span><span class="n">c</span><span class="p">)[:,</span> <span class="bp">None</span><span class="p">].</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">modulate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="nf">norm_final</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">shift</span><span class="o">=</span><span class="n">shift</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">masked</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>

        <span class="n">add_token</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">masked</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="n">self</span><span class="p">.</span><span class="n">vocab_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="n">add_token</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">time_embedding</span> <span class="o">=</span> <span class="nc">TimestepEmbedder</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">cond_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="nc">Rotary</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">//</span> <span class="n">config</span><span class="p">.</span><span class="n">n_heads</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="nc">DDiTBlock</span><span class="p">(</span>
                    <span class="n">dim</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span>
                    <span class="n">n_heads</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">n_heads</span><span class="p">,</span>
                    <span class="n">cond_dim</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">cond_dim</span><span class="p">,</span>
                    <span class="n">dropout</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">dropout</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">n_blocks</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="nc">DDitFinalLayer</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">vocab_size</span> <span class="o">+</span> <span class="n">add_token</span><span class="p">,</span>
            <span class="n">cond_dim</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">cond_dim</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">time</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">vocab_embed</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">time_embedding</span><span class="p">(</span><span class="n">time</span><span class="o">=</span><span class="n">time</span><span class="p">))</span>

        <span class="n">rotary_cos_sin</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rotary_emb</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">)):</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">rotary_cos_sin</span><span class="o">=</span><span class="n">rotary_cos_sin</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>

            <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">output_layer</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>Training</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">msafile</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1hxe.a2m</span><span class="sh">'</span>
<span class="n">msa</span> <span class="o">=</span> <span class="nf">msa_to_torch</span><span class="p">(</span><span class="n">msafile</span><span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">train</span> <span class="o">=</span> <span class="bp">False</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="nc">PolynomialScheduler</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">path</span> <span class="o">=</span> <span class="nc">MixtureDiscreteProbPath</span><span class="p">(</span><span class="n">scheduler</span><span class="o">=</span><span class="n">scheduler</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">n_blocks</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-3</span>

<span class="n">seq_models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">source_loss_combo</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">),</span> 
                     <span class="p">(</span><span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">)]</span>

<span class="nf">for </span><span class="p">(</span><span class="n">source_distribution</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">)</span> <span class="ow">in</span> <span class="n">source_loss_combo</span><span class="p">:</span>

    <span class="c1"># training arguments
</span>    <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">"</span><span class="s">uniform</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">added_token</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">"</span><span class="s">mask</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">mask_token</span> <span class="o">=</span> <span class="n">vocab_size</span>  <span class="c1"># tokens starting from zero
</span>        <span class="n">added_token</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span>

    <span class="c1"># additional mask token
</span>    <span class="n">vocab_size</span> <span class="o">+=</span> <span class="n">added_token</span>

    <span class="c1"># probability denoiser model init
</span>    <span class="c1"># Model initialization
</span>    <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">'</span><span class="s">config.yaml</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">n_blocks</span> <span class="o">=</span> <span class="n">n_blocks</span>
    <span class="n">cfg</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.3</span>
    <span class="n">probability_denoiser</span> <span class="o">=</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">cfg</span><span class="p">.</span><span class="n">model</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">masked</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Number of parameters =</span><span class="sh">'</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">probability_denoiser</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()))</span>

    <span class="c1"># init optimizer
</span>    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">probability_denoiser</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span> 
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">path</span><span class="p">)</span> <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">KL</span><span class="sh">'</span> <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>

    <span class="c1"># train
</span>    <span class="k">if</span> <span class="n">train</span><span class="p">:</span> 
        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

        <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">pbar</span> <span class="o">=</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">pbar</span><span class="p">:</span>

            <span class="c1"># sample data (user's responsibility): in this case, (X_0,X_1) ~ pi(X_0,X_1)
</span>            <span class="n">x_1</span> <span class="o">=</span> <span class="nf">inf_seq_train_gen</span><span class="p">(</span><span class="n">msa</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># sample data
</span>
            <span class="k">if</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">"</span><span class="s">uniform</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">source_distribution</span> <span class="o">==</span> <span class="sh">"</span><span class="s">mask</span><span class="sh">"</span><span class="p">:</span>
                <span class="n">x_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">mask_token</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nb">NotImplementedError</span>

            <span class="c1"># sample time (user's responsibility)
</span>            <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">x_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>

            <span class="c1"># sample probability path
</span>            <span class="c1"># mixture of discrete probability path for each token
</span>            <span class="n">path_sample</span> <span class="o">=</span> <span class="n">path</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">x_0</span><span class="o">=</span><span class="n">x_0</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">)</span>

            <span class="c1"># discrete flow matching generalized KL loss
</span>            <span class="n">logits</span> <span class="o">=</span> <span class="nf">probability_denoiser</span><span class="p">(</span><span class="n">path_sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">path_sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">KL</span><span class="sh">'</span><span class="p">:</span> 
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">x_1</span><span class="o">=</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_t</span><span class="o">=</span><span class="n">path_sample</span><span class="p">.</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">path_sample</span><span class="p">.</span><span class="n">t</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">CE</span><span class="sh">'</span><span class="p">:</span> 
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">einops</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="sh">'</span><span class="s">b n d -&gt; (b n) d</span><span class="sh">'</span><span class="p">),</span> 
                               <span class="n">einops</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="sh">'</span><span class="s">b n -&gt; (b n)</span><span class="sh">'</span><span class="p">))</span><span class="c1"># This should be consistent with the following:
</span>            <span class="c1"># logit_to_velocity(pred_x_1, x_t, t) - logit_to_velocity(x_1, x_t, t)
</span>
            <span class="c1"># optimizer step
</span>            <span class="n">optim</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> 
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># backward
</span>            <span class="n">optim</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># update
</span>
            <span class="n">pbar</span><span class="p">.</span><span class="nf">set_description</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">loss = </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>    
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">probability_denoiser</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sa">f</span><span class="sh">'</span><span class="s">seq_</span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s">.pth</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span> 
        <span class="n">probability_denoiser</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">seq_</span><span class="si">{</span><span class="n">source_distribution</span><span class="si">}</span><span class="s">_</span><span class="si">{</span><span class="n">loss_type</span><span class="si">}</span><span class="s">.pth</span><span class="sh">'</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    
    <span class="n">seq_models</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">copy</span><span class="p">.</span><span class="nf">deepcopy</span><span class="p">(</span><span class="n">probability_denoiser</span><span class="p">))</span>
</code></pre></div></div> <p>Sampling</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="o">%%</span><span class="n">time</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">22</span>
<span class="n">seq</span> <span class="o">=</span> <span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">RESTYPES_WITH_X_GAP</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">msa</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">seqs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">source_distribution</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">source_loss_combo</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">seq_models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">sol</span> <span class="o">=</span> <span class="nf">sample_discrete_flow_matching_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> 
                                              <span class="n">source_distribution</span><span class="o">=</span><span class="n">source_distribution</span><span class="p">,</span> 
                                              <span class="n">solver_type</span><span class="o">=</span><span class="sh">'</span><span class="s">Heun</span><span class="sh">'</span><span class="p">,</span> 
                                              <span class="n">dim</span><span class="o">=</span><span class="n">msa</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> 
                                              <span class="n">n_samples</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">nfe</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">n_plots</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">sols</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">seqs</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="sh">''</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">RESTYPES_WITH_X_GAP</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">s</span><span class="p">])</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sol</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    
    <span class="nf">print</span><span class="p">()</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">source_distribution</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Original</span><span class="sh">'</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Samples</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="nf">print</span><span class="p">(</span><span class="n">seqs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
    <span class="nf">print</span><span class="p">()</span>
    
</code></pre></div></div> <hr> <p>Sampling with Heun solver</p> <p>uniform KL</p> <p><code class="language-plaintext highlighter-rouge">Original</code></p> <p>IVEGSDAEIGMSPWQVMLFRKSPQELLCGASLISDRWVLTAAHCLLYPPWDKNFTENDLLVRIGKHSRTRYERNIEKISMLEKIYIHPRYNWRENLDRDIALMKLKKPVAFSDYIHPVCLPDRETAASLLQAGYKGRVTGWGNLKETWTANVGKGQPSVLQVVNLPIVERPVCKDSTRIRITDNMFCAGYKPDEGKRGDACEGDSGGPFVMKSPFNNRWYQMGIVSWGEGCDRDGKYGFYTHVFRLKKWIQKVIDQFGE</p> <p><code class="language-plaintext highlighter-rouge">Samples</code></p> <p>IVGGANAPAGSWPWQVSLQING–GHFCGGSLINNEWVLSAAHCFPS——-STSGIQVNLGRQNLQGSNPN-EVFRSVSTIIIHPNYNS-DSNDNDIALLRLSSPVTFNNYISPVCLAASG—STFHNGTDCWVTGFGDIRSD—-VPLPFPNTLQEVQVPVIGNRQCNCNYGGSITGNMICAGL———————————————————————</p> <p>IVGGSEAELGEWPWQVSLRYNR–SHICGGALVSDKWILSAAHCFEEY—–RDPAEWKVYMGLYSQDSLNK–YKGISVKQIISHPNYNP-ETKDYDIALLQLEEPVLYTNFVQPICLPRSG—HVFPPGTICWITGWGRIQEE——GSSSNALQKAMVPIIDRHFCSRLYPSGIKPGMICAGFI–EGG-VDACQGDSGGPLVCKE-KGSIFFLAGITSWGIGCGLPNKPGVYTRVTELNSWIREKM—–</p> <p>IVGGSAAEISTYPWQVSLTSGG–RHFCGGSVVAPKIVLTAAHCVVG——-QPSSIRVRVGRTDKATGGG—QIISVSEQWIHPKYND-NTNDGDWALIKLAQPIAYSPAIQTISLATTA—–YAAGTTATVSGWGATTGT——GDYANTLRAVAVPLVSDTECRAAYPGDLTDNMVCAGYL–DGG-RDACQGDSGGPLVAGG——KLVGLVSWGYGCGQAGKPGVYTEVS—————</p> <p>IVGGEDAPAGSWPWQVSLHTFG—HFCGGSLINNEWVVTAAHCFSR—————LGRHSLEGSNPN-EQSLSVSRVIKHPNYDS-STNDNDICLLQLQSPVTLTNYVRPVCLAASG—SVFANGTNSWVTGWGNTAEG—-VSLPFPANLQEVEVPVLGNRQCKCLYGSTITNNMICAGLL–AGG-KDSCQGDSGGPMVSKN–NSVWIQSGVVSWGYGCALPNYPGVYTRVSEYQSWINSQI—–</p> <p>IVGGEDAPAGSWPWQVSLHTFG–GHFCGGSLINKEWVLSAAHCFQS——WSTAGWEVYLGRQSLQGNNPN-EQSRTVSKIIIHPNYDS-RTNDNDIALLQLSSPVTFNNYIRPVCLAAFG—SVFNSGTSSWVTGWGNVEEG———PDTLMEVMVPVVGNRQCNCLYGVTITNNMICAGYL–AGG-KDSCQGDSGGPLVSKQ–GSRWVQAGIVSFGIGCAQPNKPGVYARVSRYQTWINSNI—–</p> <hr> <p>Sampling with Heun solver</p> <p>mask CE</p> <p><code class="language-plaintext highlighter-rouge">Original</code></p> <p>IVEGSDAEIGMSPWQVMLFRKSPQELLCGASLISDRWVLTAAHCLLYPPWDKNFTENDLLVRIGKHSRTRYERNIEKISMLEKIYIHPRYNWRENLDRDIALMKLKKPVAFSDYIHPVCLPDRETAASLLQAGYKGRVTGWGNLKETWTANVGKGQPSVLQVVNLPIVERPVCKDSTRIRITDNMFCAGYKPDEGKRGDACEGDSGGPFVMKSPFNNRWYQMGIVSWGEGCDRDGKYGFYTHVFRLKKWIQKVIDQFGE</p> <p><code class="language-plaintext highlighter-rouge">Samples</code></p> <p>IVGGSEATPGSHPWQAALYISPAEKVFCGGSLIDKCWVATAAHCFKDE——REQYVTVVLGDHHLNRTEGS-EQSLKVEEAIIHPCYNP-SSYDSDIALLKLKHPAKLSKAVSPVCLPEET—QIFSAGSECTISGWGQTEEG—–ADSYSVVLQEAQVPLIDQEQCSKPYGTELDENMMCAGYM–EGG-ADSCQGDSGGPLTCQW–DGRMFLLGITSWGYGCAKPNKPGVYTRVTNFSEWIQSTT—–</p> <p>VVGGYEAVQSKLPYNVSIQQGQSNSHFCSGALINERWVLTAAHCVMRR—YHLPRNQLEAVLGTHKLTSGGSL-GQTRRVTTIIRHPDGKDVCKYRSNIALIELNPKVNF–KVQPIRISDED—–LTPNTKCIVAGWGITKAG——-GEVLPLNKATVPYVNERACKEYHLEFLGKETLCVGHD–QGL-RGVCDGDAGGGLFCKT-SNDPWKLTGIAVGGQEPCSFTGPSIYIDIRHHLEWLMQNI—–</p> <p>VAGGNDGRPGAHPWIVALFRNG–THFCGGSLIKGSWVLSAAHCFYNH—-NTDGSDLVAIVGDHQLNRHDGE-EVLVAVSGVIMNQQYNP-NTLQYDIALIKLVQPVSFTEYIQPICLPSPR—VELNENRVCTVTGWGTTQPG—-APPLVSNPLQSVAVPVQATGDCKAAYSHSITDRMLCAGYR–EGN-KDSCQGDSGGPLLCRN–GEQYELHGVVSWGFGCGHPDFYAVYVRTSYLIQWINQTT—–</p> <p>IVGGADTTINQYPAQVSLLISSGGWHFCGGSIINNRWILTGAHCSHA——-SPNFRRVRVGSSFASEGG—–VHNVERIIVHEGYDW-LTHDNDISVLRLSTALTFSNNIQPAPIAGAN—TTVGENDAAWAAGWGATANG——GGSENALQHVQVPVVNQRQCRRNYANRITNNMICSGWL-GAGG-RDSCQGDSGGPLTHNG——TLVGVCSFGIGCALRRYPGVYARVSSYSSWIDAN——</p> <p>IIGGRLVTNESRPYQVSLRKEDSKRHSCGGFLISERFALTAAHCNLEP-RSFGQVPALTNVRVGSSFTSSGG—–LHPVRRLIVHPNYDE-QTLDHDIRLLQLDRKVHLNDTVRVVSLPDSP—-DVEDNTLCTTSGWGTTEPDTVKSG-IERPDELRELKLTILNA-ACARQ——-RHLCTGVP–KRE-SGPCAGDSGGPLVCNG——PVHGVASYSRNCG——-FTKIATYVTWLLGQT—–</p> <p>CPU times: user 49.7 s, sys: 5.29 ms, total: 49.7 s</p> <p>Wall time: 49.8 s</p> <p>For serine protease, the first couple of residues are critical for substrate recognition and binding. These residues often form the active site and are highly conserved across different species. In the sample, many of the sequences has similar <code class="language-plaintext highlighter-rouge">IVEGS</code> and <code class="language-plaintext highlighter-rouge">PWQV</code> motifs from the uniform KL track. The mask CE track also captures these motifs, but with more variability. Uniform source distribution is therefore preferred by many DFM models empirically.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/dfm/out.gif-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/dfm/out.gif-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/dfm/out.gif-1400.webp"></source> <img src="/assets/img/posts/dfm/out.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Next, we want to assess the ELBO score for 512 sequences:</p> <p>128 from the training set, index 0 being the query <code class="language-plaintext highlighter-rouge">1hxe.pdb</code> sequence</p> <p>128 sampled sequences</p> <p>128 from the training data, with 10 N-term residue randomly mutated</p> <p>128 random sequence.</p> <p>As the following histogram shows, the sampled sequences have the highest ELBO and logP/dim (preferred), and the random sequences being highly unlikely with the most negative ELBO estimates.</p> <p>When the first 10 residues are mutated (highly preserved regions), the ELBO dropped, suggesting that the ELBO score can be used to gauge the quality of the sequence.</p> <p>The first spike was from the query, as most of the sequences in the MSA contain gap, a no-gap query sequence then becomes an outlier with high ELBO scores. One proper way to do this is to remove the gaps in the MSA or masked the gap prediction in the loss.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># compute the elbo and logP for original seq, samples and random seqs
</span>
<span class="c1"># Generalized KL function (will use it to compute the elbo)
</span><span class="n">generalized_kl_fn</span> <span class="o">=</span> <span class="nc">MixturePathGeneralizedKL</span><span class="p">(</span><span class="n">path</span> <span class="o">=</span> <span class="n">path</span><span class="p">,</span> <span class="n">reduction</span> <span class="o">=</span><span class="sh">'</span><span class="s">none</span><span class="sh">'</span><span class="p">)</span>
<span class="n">elbo_dcts</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">source_distribution</span><span class="p">,</span> <span class="n">loss_type</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">source_loss_combo</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">seq_models</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
    <span class="c1"># first 10 position random mutation
</span>    <span class="n">mut</span> <span class="o">=</span> <span class="n">msa</span><span class="p">[:</span><span class="mi">128</span><span class="p">].</span><span class="nf">clone</span><span class="p">()</span>
    <span class="n">mut</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint_like</span><span class="p">(</span><span class="n">mut</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">10</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
    
    <span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">msa</span><span class="p">[:</span><span class="mi">128</span><span class="p">],</span> 
                     <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">sols</span><span class="p">[</span><span class="n">i</span><span class="p">][:</span><span class="mi">128</span><span class="p">]),</span>
                     <span class="n">mut</span><span class="p">,</span>
                     <span class="n">torch</span><span class="p">.</span><span class="nf">randint_like</span><span class="p">(</span><span class="n">msa</span><span class="p">[:</span><span class="mi">128</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)]).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">elbo_dcts</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">elbo_estimate</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">source_distribution</span><span class="p">))</span>



<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">127</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span> <span class="o">+</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="mi">128</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span> 
    <span class="n">my_cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">get_cmap</span><span class="p">(</span><span class="sh">'</span><span class="s">tab10</span><span class="sh">'</span><span class="p">)</span>
<span class="c1">#     rescale = lambda y: (y - np.min(y)) / (np.max(y) - np.min(y))
</span>    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">bar</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">512</span><span class="p">),</span> <span class="n">elbo_dcts</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">logp_per_dim</span><span class="sh">'</span><span class="p">].</span><span class="nf">cpu</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="nf">my_cmap</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="n">source_loss_combo</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">seq #</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">logP/dim</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</code></pre></div></div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/posts/dfm/fig3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/posts/dfm/fig3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/posts/dfm/fig3-1400.webp"></source> <img src="/assets/img/posts/dfm/fig3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Yen-Lin Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: June 27, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>